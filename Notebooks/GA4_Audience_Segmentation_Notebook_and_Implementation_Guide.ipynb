{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tpwX9ySYHqwm",
      "metadata": {
        "id": "tpwX9ySYHqwm"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# Author : Lavi Nigam, ML Engineering @ Google\n",
        "# Linkedin: https://www.linkedin.com/in/lavinigam/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aBoKd8UlHuMk",
      "metadata": {
        "id": "aBoKd8UlHuMk"
      },
      "source": [
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/lavinigam-gcp/AI-ML-Patterns/blob/main/Notebooks/GA4_Audience_Segmentation_Notebook_and_Implementation_Guide.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0462bd41-ac34-4250-9307-3843e25f2a87",
      "metadata": {
        "id": "0462bd41-ac34-4250-9307-3843e25f2a87"
      },
      "source": [
        "# E-Commerce Audience Segmentation using using Google Analytics (GA4) Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1bca8e3-ea6f-4a68-8358-659ff9fc6df0",
      "metadata": {
        "id": "f1bca8e3-ea6f-4a68-8358-659ff9fc6df0"
      },
      "source": [
        "Learn how to build a system to create audience segmentation of GA4 e-commerce data using BigQuery ML (BQML)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16051aa1-62b0-44a5-83ae-dd038f849eee",
      "metadata": {
        "id": "16051aa1-62b0-44a5-83ae-dd038f849eee"
      },
      "source": [
        "With recent changes, BigQuery ML can directly access GA4 data, bringing capture app and web data in a single interface. This integration opens many opportunities for various machine learning use cases and potential customers. For example, the e-commerce industry can funnel their GA4 data to BQML and expand their analytics with ML capabilities. This pattern aims to help such companies leverage different ML algorithms and scale their analytics with BQ."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56691fb0-e600-45e9-8e27-b80b360d9e8e",
      "metadata": {
        "id": "56691fb0-e600-45e9-8e27-b80b360d9e8e"
      },
      "source": [
        "Customer segmentation, or grouping customers based on common characteristics, allows e-commerce businesses to provide relevant and timely customer promotions and offers. For example, customers buying behaviors are influenced by their demographics, surfing habits, interests, and even the gadgets they use. The buying behavior will impact what they buy, why they buy, and how much money they spend on each purchase.\n",
        "\n",
        "When combined with behavioral data, customer segmentation allows online retailers to provide tailored experiences similar to those found in a customer's favorite neighborhood store."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6042ad4-3a29-4e58-910c-5fe6fd7d5df1",
      "metadata": {
        "id": "f6042ad4-3a29-4e58-910c-5fe6fd7d5df1"
      },
      "source": [
        "Customers can use k-means clustering, included in BQML, on their e-commerce GA4 data to quickly create customer segments. Clustering will allow users of similar behaviors in GA4 to be segmented, personalizing client outreach by adapting ads and other messages to their tastes and habits, as evidenced by the cluster each customer belongs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZNooLB1Wq9Vf",
      "metadata": {
        "id": "ZNooLB1Wq9Vf"
      },
      "source": [
        "One thing to note is that GA4 has an [built-in predictive audience feature](https://support.google.com/analytics/answer/9805833) that helps segment customers based on churn, purchases, and spending. However, the solution discussed in the pattern is different from that in the sense that it tries to bring more dynamic features like user demographic and interests (page view based). The inbuilt feature is more static, while the pattern is more dynamic since it uses machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a50caf5-6a68-4c81-9b4a-613b66a600d6",
      "metadata": {
        "id": "3a50caf5-6a68-4c81-9b4a-613b66a600d6"
      },
      "source": [
        "## Objective\n",
        "\n",
        "The design pattern involves the following steps:\n",
        "\n",
        "- Loading the e-commerce data\n",
        "- Exploratory analysis of the data\n",
        "- Feature engineering and pre-processing of the data for model\n",
        "- Create, train, and deploy the model in BigQuery\n",
        "- Evaluate the model to understand various clusters\n",
        "- Batch prediction using the trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lwYLPV4xUuAC",
      "metadata": {
        "id": "lwYLPV4xUuAC"
      },
      "source": [
        "## Audience\n",
        "The pattern is intended for marketing analytics teams for an enterprise, or, teams explicitly responsible for analyzing Google Analytics data. It assumes that you have basic knowledge of the following:\n",
        "\n",
        "- Machine learning concepts\n",
        "- Standard SQL & Python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9150b5b9-b8a0-443e-ac34-77033acec59a",
      "metadata": {
        "id": "9150b5b9-b8a0-443e-ac34-77033acec59a"
      },
      "source": [
        "## Costs\n",
        "This tutorial uses the following billable components of Google Cloud:\n",
        "\n",
        "- BigQuery\n",
        "- BigQuery ML\n",
        "- Cloud Storage\n",
        "\n",
        "\n",
        "To generate a cost estimate based on your projected usage, use the pricing calculator.\n",
        "\n",
        "Learn about\n",
        "- [BigQuery\n",
        "pricing](https://cloud.google.com/bigquery/pricing),\n",
        "- [BigQuery ML pricing](https://cloud.google.com/bigquery-ml/pricing),\n",
        "- [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing),\n",
        "\n",
        "and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nSxUki7YZjJt",
      "metadata": {
        "id": "nSxUki7YZjJt"
      },
      "source": [
        "## The Dataset\n",
        "The solution uses the public [GA4 Google Merchandise Store](https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=ga4_obfuscated_sample_ecommerce) dataset.\n",
        "\n",
        "Google Merchandise Store is an online store that sells Google-branded merchandise. The site uses Google Analytics 4's standard web ecommerce implementation along with enhanced measurement. The ga4_obfuscated_sample_ecommerce dataset available through the BigQuery Public Datasets program and contains a sample of obfuscated BigQuery event export data for three months from 2020-11-01 to 2021-01-31.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jx5ZSKnvw5fO",
      "metadata": {
        "id": "jx5ZSKnvw5fO"
      },
      "source": [
        "This dataset contains obfuscated data that emulates what a real world dataset would look like from an actual Google Analytics 4 implementation. Certain fields will contain placeholder values including <Other>, NULL, and \" \" . Due to obfuscation, internal consistency of the dataset might be somewhat limited.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Du_xhjb2w8UF",
      "metadata": {
        "id": "Du_xhjb2w8UF"
      },
      "source": [
        "You can check the schema details of the dataset [here](https://support.google.com/analytics/answer/7029846#zippy=)\n",
        "\n",
        "There are total 23 columns in the datasets with mixed datatypes, and approximately 4 million rows (each day event is seperate table in the data and total 92 events(tables) are present)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lxvH0lhKdsP4",
      "metadata": {
        "id": "lxvH0lhKdsP4"
      },
      "source": [
        "## Exporting Google Analytics data to BigQuery\n",
        "If instead of the sample data, you want to use your own data from a GA4 property, you can follow the instructions in [(GA4) Set up BigQuery ](https://support.google.com/analytics/answer/9823238#zippy=%2Cin-this-article) Export to export your data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3Ro0D7_NIJ9t",
      "metadata": {
        "id": "3Ro0D7_NIJ9t"
      },
      "source": [
        "### Set up your local development environment\n",
        "\n",
        "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
        "all the requirements to run this notebook. You can skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "naVsQIkoIKqi",
      "metadata": {
        "id": "naVsQIkoIKqi"
      },
      "source": [
        "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
        "You need the following:\n",
        "\n",
        "* The Google Cloud SDK\n",
        "* Python 3\n",
        "* Jupyter notebook running in a virtual environment with Python 3\n",
        "\n",
        "The Google Cloud guide to [Setting up a Python development\n",
        "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
        "installation guide](https://jupyter.org/install) provide detailed instructions\n",
        "for meeting these requirements. The following steps provide a condensed set of\n",
        "instructions:\n",
        "\n",
        "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
        "\n",
        "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
        "\n",
        "1. [Install\n",
        "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
        "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
        "\n",
        "1. To install Jupyter, run `pip3 install jupyter` on the\n",
        "command-line in a terminal shell.\n",
        "\n",
        "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
        "\n",
        "1. Open this notebook in the Jupyter Notebook Dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-kMgGax-IS1F",
      "metadata": {
        "id": "-kMgGax-IS1F"
      },
      "source": [
        "### Install additional packages\n",
        "\n",
        "Install additional package dependencies not installed in your notebook environment, such as {plotly.express, pandas, google.cloud}. Use the latest major GA version of each package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IeWQObjWmkel",
      "metadata": {
        "id": "IeWQObjWmkel"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:s\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "! pip3 install --upgrade google-cloud-aiplatform {USER_FLAG} -q google-cloud-bigquery db-dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cmiF7wD2IXbk",
      "metadata": {
        "id": "cmiF7wD2IXbk"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ng8iy7LaIa9q",
      "metadata": {
        "id": "Ng8iy7LaIa9q"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bKoNzVCxIf7O",
      "metadata": {
        "id": "bKoNzVCxIf7O"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EFwFJmXZIgqV",
      "metadata": {
        "id": "EFwFJmXZIgqV"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "\n",
        "3. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "4. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WbtKuolkIo2k",
      "metadata": {
        "id": "WbtKuolkIo2k"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SsjOE5jkIsuU",
      "metadata": {
        "id": "SsjOE5jkIsuU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"\"\n",
        "\n",
        "# Get your Google Cloud project ID from gcloud\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID: \", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S8aHk0AQIxlM",
      "metadata": {
        "id": "S8aHk0AQIxlM"
      },
      "source": [
        "Otherwise, set your project ID here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kbAokgs1Iz_o",
      "metadata": {
        "id": "kbAokgs1Iz_o"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
        "    PROJECT_ID = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vafnp9fDI-Jy",
      "metadata": {
        "id": "vafnp9fDI-Jy"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Google Cloud Notebooks**, your environment is already\n",
        "authenticated. Skip this step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KmPLvdx-JFti",
      "metadata": {
        "id": "KmPLvdx-JFti"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# If on Google Cloud Notebooks, then don't execute this code\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cf783db-023f-41d1-8753-b194ccefc759",
      "metadata": {
        "id": "5cf783db-023f-41d1-8753-b194ccefc759"
      },
      "outputs": [],
      "source": [
        "# Importing some important libraries that will be used during the notebook\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u4AoPtwKSNhm",
      "metadata": {
        "id": "u4AoPtwKSNhm"
      },
      "outputs": [],
      "source": [
        "# Client manages connections to the BigQuery API and helps\n",
        "# bundle configuration (project, credentials) needed for API requests.\n",
        "client = bigquery.Client(PROJECT_ID)\n",
        "\n",
        "# to make sure all columns are displayed while working with dataframe\n",
        "pd.set_option(\"display.max_columns\", None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2_BpasBjb3eG",
      "metadata": {
        "id": "2_BpasBjb3eG"
      },
      "source": [
        "## Assumptions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oMutkEaqhJsJ",
      "metadata": {
        "id": "oMutkEaqhJsJ"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Jj4q_owsvDI8",
      "metadata": {
        "id": "Jj4q_owsvDI8"
      },
      "source": [
        "You can start by defining some essential variables that can change according to your data. It is always better to consider the most recent records from your data as features. For this purpose, you can set the START_DATE and END_DATE based on your data recency.\n",
        "\n",
        "In this case, the date range is set for 3 months."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YI1pDBZMu5hm",
      "metadata": {
        "id": "YI1pDBZMu5hm"
      },
      "outputs": [],
      "source": [
        "# Dataset (GA4 Google Merchandise Store) specific Variable\n",
        "# Change it to your dataset spefic values, if you want to use the code for your data.\n",
        "# We assume table names will be \"events_*\"\n",
        "\n",
        "PROJECT_ID_DATA = \"bigquery-public-data\"\n",
        "DATASET_ID_DATA = \"ga4_obfuscated_sample_ecommerce\"  # ga4-bq-pattern.1crdata.fake_ga4 #ga4_obfuscated_sample_ecommerce\n",
        "START_DATE = \"20201101\"\n",
        "END_DATE = \"20210131\"  # taking 3 months recent data.\n",
        "\n",
        "# In queries, these variables are editable so that you can put your project, dataset, and date,\n",
        "# making it easier for you to make the least amount of changes. Of course, you don't need to change\n",
        "# it for public data. But, for making the queries editable, it made sense to define them here.\n",
        "# You can run the whole notebook (mostly) with your data by changing values here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rzXCHmAdLKK-",
      "metadata": {
        "id": "rzXCHmAdLKK-"
      },
      "source": [
        "### Why EDA?\n",
        "\n",
        "Before starting the audience segmentation, it's essential to identify the various attributes commonly referred to as features (in ML),  based on which the intended audience should be segmented. For example, you can segment your customers based on different languages, locations, pages, and interests on your website, mobile company, shipping methods, etc.\n",
        "\n",
        "So, with exploratory data analysis (EDA), the goal here is to:\n",
        "\n",
        "- Find the value distribution, availability, and data types of columns.\n",
        "- Aggregate summary statistics for important columns.\n",
        "- Identify critical columns to be used as features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9613d67-35cd-40c3-b7e4-f08926c7594f",
      "metadata": {
        "id": "b9613d67-35cd-40c3-b7e4-f08926c7594f"
      },
      "source": [
        "You can start the data exploration by returning the first five rows of data.\n",
        "The data has multiple event tables for each day. So, all the tables (events) could be queried by using events* as the wildcard.\n",
        "\n",
        "[GA4 Data Export Schema](https://support.google.com/analytics/answer/7029846#zippy=)\n",
        "\n",
        "Note: BigQuery export, by default, are [date sharded tables](https://cloud.google.com/bigquery/docs/partitioned-tables#dt_partition_shard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35270b17-147e-495f-8839-ffe363db0aa7",
      "metadata": {
        "id": "35270b17-147e-495f-8839-ffe363db0aa7"
      },
      "outputs": [],
      "source": [
        "query = f\"\"\"\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  `{PROJECT_ID_DATA}.{DATASET_ID_DATA}.events*`\n",
        "LIMIT\n",
        "  5\n",
        "\"\"\"\n",
        "query_job = client.query(query)\n",
        "top5_data = query_job.to_dataframe()\n",
        "top5_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e665b736-7c46-4131-a1fa-d050d40e30f4",
      "metadata": {
        "id": "e665b736-7c46-4131-a1fa-d050d40e30f4"
      },
      "source": [
        "The first five rows of data can help you understand the tables' composite structure of data types. For example, you can see numerical, categorical, arrays, and struct as data types."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MXy4Sy-raeEr",
      "metadata": {
        "id": "MXy4Sy-raeEr"
      },
      "source": [
        "By looking at some columns, you can also identify a few essential features like event_date, event_name, user_ltv, device, geo, traffic_source, platform, and items. However, as discussed earlier, you still are not aware of their value distribution, availability, and data types."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qRGBRNZRYg7A",
      "metadata": {
        "id": "qRGBRNZRYg7A"
      },
      "source": [
        "You can check the data types of each column using [INFORMATION_SCHEMA](https://cloud.google.com/bigquery/docs/information-schema-tables) table. It can give you detailed metadata of your columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rgE--AsdTJnd",
      "metadata": {
        "id": "rgE--AsdTJnd"
      },
      "outputs": [],
      "source": [
        "query = f\"\"\"\n",
        "SELECT\n",
        "  DISTINCT(column_name),\n",
        "  data_type\n",
        "FROM\n",
        "  `{PROJECT_ID_DATA}.{DATASET_ID_DATA}.INFORMATION_SCHEMA.COLUMNS`\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "predict_data = query_job.to_dataframe()\n",
        "predict_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G29DgyP4vjgV",
      "metadata": {
        "id": "G29DgyP4vjgV"
      },
      "source": [
        "You can start by understanding overall data by getting a quick summary of the data, namely - total events  (event_count), total users (user_count), total days in the data (day_count), and total registered users of the platform (registered_user_id)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rYSCfLhr6dUl",
      "metadata": {
        "id": "rYSCfLhr6dUl"
      },
      "outputs": [],
      "source": [
        "query = f\"\"\"\n",
        "\n",
        "SELECT\n",
        "  COUNT(*) AS event_count,\n",
        "  COUNT(DISTINCT user_pseudo_id) AS user_count,\n",
        "  COUNT(DISTINCT event_date) AS day_count,\n",
        "  COUNT(DISTINCT user_id) AS registered_user_id\n",
        "FROM\n",
        "  `{PROJECT_ID_DATA}.{DATASET_ID_DATA}.events*`\n",
        "\"\"\"\n",
        "query_job = client.query(query)\n",
        "top5_data = query_job.to_dataframe()\n",
        "top5_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "trhmmrbiS334",
      "metadata": {
        "id": "trhmmrbiS334"
      },
      "source": [
        "As you can observe, there are roughly 4 million events with close to 270,000 users, stretched along 92 days of activity on the platform.\n",
        "\n",
        "For simplicity, we will **assume** that all user_pseudo_id are unique and represent a single user.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37fx1NhXMINA",
      "metadata": {
        "id": "37fx1NhXMINA"
      },
      "source": [
        "Some potential columns of interest are already identified in our exploration, like - event_date, event_name, user_ltv, device, geo, traffic_source, platform, and items."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J4a69In4lqMh",
      "metadata": {
        "id": "J4a69In4lqMh"
      },
      "source": [
        "---\n",
        "Now, you can start by looking into `event_name` distribution.\n",
        "\n",
        "event_name is a significant column in this dataset. It contains all the events triggered as users interact with the Google Merchandise Store like page_view, scroll (scrolling the page), view_item (viewing specific item), etc. You can refer [here](https://developers.google.com/analytics/devguides/collection/ga4/reference/events) for a more detailed meaning of each event_name.\n",
        "\n",
        "As you can see, it is easy to visualize grouped data in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DbiCdEaxMCBY",
      "metadata": {
        "id": "DbiCdEaxMCBY"
      },
      "outputs": [],
      "source": [
        "query = f\"\"\"\n",
        "SELECT\n",
        "  event_name,\n",
        "  COUNT(*) as row_count\n",
        "FROM\n",
        "   `{PROJECT_ID_DATA}.{DATASET_ID_DATA}.events*`\n",
        "GROUP BY\n",
        "  event_name\n",
        "ORDER BY\n",
        "  row_count DESC\n",
        "\"\"\"\n",
        "query_job = client.query(query)\n",
        "result_df = query_job.to_dataframe()\n",
        "fig = px.bar(\n",
        "    result_df, x=\"row_count\", y=\"event_name\", title=\"Event Name Frequency Distribution\"\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m1spMS_ysKE0",
      "metadata": {
        "id": "m1spMS_ysKE0"
      },
      "source": [
        "You can observe a great imbalance in the frequency of different event_name(s). The top five events based on frequency:\n",
        "\n",
        "* page_view - User is viewing a page\n",
        "\n",
        "* user_engagement - Sessions that last 10 seconds or longer\n",
        "\n",
        "* scroll - User scrolling through a page\n",
        "\n",
        "* view_item - some content was shown to the user. You can use this to discover the most popular items.\n",
        "\n",
        "* session_start - User session after the engagement has been initiated.\n",
        "\n",
        "\n",
        "The other events don't have too many records and hence would be a challenge to be considered a feature. However, you can also notice that typical purchase events - \"add_to_cart\", \"begin_checkout\", \"add_shipping_info\", \"add_payment_info\", and \"purchase\" have a tiny amount of records, indicating that this data doesn't contain too many events where a user has bought something.\n",
        "\n",
        "\n",
        "So,`page_view` seems to be the best filter for the column `event_name` since it has the highest records and covers users' general browsing behavior. However, you can still leverage `add_to_cart` and `purchase` value for purchase information by simply counting a user's total events for these event types. So, although they will be small, you can include them for demonstration purposes to add a little more diversification in the data for the K-Means Model.\n",
        "\n",
        "Also, remember that the actual key of `page_view` event_name is available in event_params, and their values are in event_params.values.{int/float/string} in nested array format.\n",
        "\n",
        "Data references:\n",
        "\n",
        "[Dimensions & Metrics](https://support.google.com/analytics/topic/11151952?hl=en&ref_topic=9228654)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LAmRM2EwyCA8",
      "metadata": {
        "id": "LAmRM2EwyCA8"
      },
      "source": [
        "---\n",
        "\n",
        "Next, you can explore event_params. There are multiple `event_params`  (shown in image) for each `event_name` ( like page_view) that stores the information for that event. It is a nested array and would require UNNEST function to access its key and different values (string, int, and float)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-M9H3Vs2SAmP",
      "metadata": {
        "id": "-M9H3Vs2SAmP"
      },
      "source": [
        "If you are unsure of the GA4 export schema; please refer [here](https://support.google.com/analytics/answer/3437719?hl=en)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W4NbomfU2LBZ",
      "metadata": {
        "id": "W4NbomfU2LBZ"
      },
      "outputs": [],
      "source": [
        "query = f\"\"\"\n",
        "SELECT\n",
        "  DISTINCT(ep.key) AS event_param_key,\n",
        "  COUNT(*) AS count\n",
        "FROM\n",
        "  `{PROJECT_ID_DATA}.{DATASET_ID_DATA}.events*`,\n",
        "  UNNEST (event_params) AS ep\n",
        "WHERE\n",
        "  event_name = 'page_view'\n",
        "GROUP BY\n",
        "  ep.key\n",
        "ORDER BY\n",
        "  count DESC\n",
        "\"\"\"\n",
        "query_job = client.query(query)\n",
        "result_df = query_job.to_dataframe()\n",
        "\n",
        "fig = px.bar(\n",
        "    result_df.head(20),\n",
        "    x=\"count\",\n",
        "    y=\"event_param_key\",\n",
        "    title=\"Event Parameters in Various Events Frequency Distribution\",\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WsgvT6T34RZP",
      "metadata": {
        "id": "WsgvT6T34RZP"
      },
      "source": [
        "In this graph, you can observe many `key` are part of event_name or a specific event type (like page_view). Moreover, you can also see their respective distributions, which can help us decide which key's to pick for our features.\n",
        "\n",
        "Among all, the following three will make sense for our current modeling:\n",
        "- page_title: page visited by a user.\n",
        "- campaign: if user event has been triggered by some campaigns.\n",
        "- engagement_time_msec: user engagement time in millisecond for the event.\n",
        "\n",
        "They will help us define user behavior on the platform. Like:\n",
        "\n",
        "- page_title can give us a view into user interest based on their browsing history,\n",
        "- campaigns can help us identify how users are engaging with the platform, and\n",
        "- engagement times can help us understand their browsing intensity.\n",
        "\n",
        "These features, together with others, can help diversify clusters. Remember, the more distinct values you have in features, the better for K-means models to segment users in different clusters.\n",
        "\n",
        "Also, one thing to note here is that we will stick to one `event_type` for these keys: ' page_view`.\n",
        "\n",
        "You can pick the others based on your business objectives and requirements, but we will stick with these for simplicity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qmALuOkXBaYk",
      "metadata": {
        "id": "qmALuOkXBaYk"
      },
      "source": [
        "---\n",
        "\n",
        "The next feature to look into is page_title. For all page_view that you filter, which a user is browsing, you can also see the page's title. This information can give you enough detail about their interest. However, a user can browse through thousands of pages, and it will be tough to find any relevant information from all of them at once.\n",
        "\n",
        "One neat way would be to find out the top most visited pages across all users and then rank each user against those pages. Like, top pages may have a title like - \"Apparel,\" \"Bags,\" \"Stationary,\" etc.\n",
        "\n",
        "In the below query, you can get the count of top most visited pages and then plot them to see the distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab9d14aa-2013-46df-b400-89a6cb63927f",
      "metadata": {
        "id": "ab9d14aa-2013-46df-b400-89a6cb63927f"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "SELECT\n",
        "  DISTINCT (ep.value.string_value) AS page_title,\n",
        "  COUNT(*) AS page_title_count\n",
        "FROM\n",
        "  `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`,\n",
        "  UNNEST (event_params) AS ep\n",
        "WHERE\n",
        "  ep.key = 'page_title'\n",
        "  AND event_name = 'page_view'\n",
        "GROUP BY\n",
        "  page_title\n",
        "ORDER BY\n",
        "  page_title_count DESC\n",
        "LIMIT\n",
        "20\n",
        "\"\"\"\n",
        "query_job = client.query(query)\n",
        "page_title_result_df = query_job.to_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a32557f6-d796-4d85-81e2-390c7105d1da",
      "metadata": {
        "id": "a32557f6-d796-4d85-81e2-390c7105d1da"
      },
      "outputs": [],
      "source": [
        "fig = px.bar(\n",
        "    page_title_result_df,\n",
        "    x=\"page_title_count\",\n",
        "    y=\"page_title\",\n",
        "    title=\"Different Page Visited Frequency Distribution\",\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZJd7sucOBmPJ",
      "metadata": {
        "id": "ZJd7sucOBmPJ"
      },
      "source": [
        "Based on the graph, you can pick four or five titles that can help with audience segmentation by adding that \"interest\" diversification, like:\n",
        "\n",
        "*   Apparel | Google Merchandise Store\n",
        "*   Bags | Lifestyle | Google Merchandise Store\n",
        "*   Hats | Apparel | Google Merchandise Store\n",
        "*   Women's | Apparel | Google Merchandise Store\n",
        "\n",
        "For this pattern, the goal is simple, and hence the focus is only to pick the top 4 or 5.\n",
        "\n",
        "In the next Feature Engineering section, we will see a way to automate this such that you wouldn't have to do this manually."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1E_xbe1sdYkr",
      "metadata": {
        "id": "1E_xbe1sdYkr"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qi7BCwzx644F",
      "metadata": {
        "id": "qi7BCwzx644F"
      },
      "source": [
        "Now that you have done some basic exploration of GA4 data, you can create different features for audience segmentation.\n",
        "\n",
        "However, before doing that, you should create a Dataset in BQ Console named \"ga4_ecomm_feature_set\" inside your project. Then, you can create a table for different kinds of features and store in the dataset.\n",
        "\n",
        "\n",
        "This will help retain the features for later purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cDVvVffhvSvC",
      "metadata": {
        "id": "cDVvVffhvSvC"
      },
      "outputs": [],
      "source": [
        "# You can create the dataset through code.\n",
        "DATASET_NAME = \"ga4_ecomm_feature_set\"\n",
        "\n",
        "try:\n",
        "    dataset = client.create_dataset(DATASET_NAME, timeout=30)  # Make an API request.\n",
        "    print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C3QPhd-5F6Qv",
      "metadata": {
        "id": "C3QPhd-5F6Qv"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Spbo4k1e111j",
      "metadata": {
        "id": "Spbo4k1e111j"
      },
      "source": [
        "The goal for feature engineering for this pattern is to create four types of features for each user:\n",
        "\n",
        "1) page_title -> top browsing interest like Apparel_visit_count, Shopping_Cart_visit_count, Mens_visit_count, YouTube_visit_count and etc.\n",
        "\n",
        "2) page_view -> total browsing events\n",
        "\n",
        "3) purchase event (add_to_cart, purchase) -> total count of add_to_cart and purchase event\n",
        "\n",
        "4) user behavior -> aggregated values of user behaviors like; favorite browser, language, device, etc., or most used country, currency, and engagement time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VpZacXEkQGWJ",
      "metadata": {
        "id": "VpZacXEkQGWJ"
      },
      "source": [
        "page_title contains the page's title viewed during the page_view (browsing) event. For example, the user might be visiting the \"Apparel\" page or \"Bags\" page.\n",
        "\n",
        "However, there can be thousands of products and their respective page_tile in a data; hence to simplify, you can only look into the top viewed page and check users' frequency for those specific pages.\n",
        "\n",
        "You can also include an exclude list while querying the top pages based on your N value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rp1OI-FsGkhd",
      "metadata": {
        "id": "rp1OI-FsGkhd"
      },
      "outputs": [],
      "source": [
        "exclude_list = (\"Home\", \"Google Online Store\")\n",
        "top_n = 20\n",
        "# query to get page_title frequency for all pages visited.\n",
        "query = f\"\"\"\n",
        "WITH\n",
        "  top_viewed_pages AS (\n",
        "  SELECT\n",
        "    DISTINCT (ep.value.string_value) AS page_title,\n",
        "    REGEXP_REPLACE(REPLACE(TRIM(SPLIT(ep.value.string_value, \"|\")[SAFE_OFFSET(0)]), \" \", \"_\"), r'[^a-zA-Z]', '') AS key,\n",
        "    COUNT(*) AS page_title_count,\n",
        "  FROM\n",
        "    `{PROJECT_ID_DATA}.{DATASET_ID_DATA}.events_*`,\n",
        "    UNNEST (event_params) AS ep\n",
        "  WHERE\n",
        "    ep.key = 'page_title'\n",
        "  GROUP BY\n",
        "    page_title\n",
        "  ORDER BY\n",
        "    page_title_count DESC\n",
        "  LIMIT\n",
        "    {top_n} )\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  top_viewed_pages\n",
        "WHERE\n",
        "  page_title NOT IN {exclude_list}\n",
        "\"\"\"\n",
        "query_job = client.query(query)\n",
        "df = query_job.to_dataframe()\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bE8nte7dZCHt",
      "metadata": {
        "id": "bE8nte7dZCHt"
      },
      "source": [
        "In the Google Merchandise Store, these are the top most visited pages cumulatively by all users. This table might look very different for your dataset and store.\n",
        "However, since we are using the general function, it won't matter as our goal is to identify the topmost pages. This query and function will give you the top-visited pages as long as you have page_title in your export schema."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w9fbw_e3bXeL",
      "metadata": {
        "id": "w9fbw_e3bXeL"
      },
      "source": [
        "The goal is to create a dynamic query such that it can take top pages visited across all users and then count the number of times the user has seen that page to generate user interest. However, the page_title text is sometimes very descriptive and may require some cleaning before being used for the dynamic query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1olnmn5u-ffG",
      "metadata": {
        "id": "1olnmn5u-ffG"
      },
      "outputs": [],
      "source": [
        "def get_query_string(page_title_keys, page_title):\n",
        "    \"\"\"\n",
        "    The function accepts:\n",
        "    page_title_keys: All the keys from page_title (extracted from the previous function) that are used as column names and variable names.\n",
        "    page_title: page_title dataframe that contains all columns\n",
        "\n",
        "    This function achieves two things:\n",
        "    1) Dynamically generates the WHEN matching query\n",
        "    2) Dynamically generates the SELECT query\n",
        "\n",
        "    case_when_string_list: Empty list that will store CASE WHEN dynamic SQL query string based on page_title keys (column names)\n",
        "    select_key_string_list: Empty list that will store SELECT dynamic SQL query string based on page_title keys (column names)\n",
        "    \"\"\"\n",
        "    case_when_string_list = []\n",
        "    select_key_string_list = []\n",
        "    for eachkey in page_title_keys:\n",
        "        select_key_string = f\"IFNULL(item_page_view_table.pageVisit_count_sum_{eachkey},0) AS {eachkey}_visit_count,\\n\"\n",
        "        select_key_string_list.append(select_key_string)\n",
        "\n",
        "    for index, row in page_title.iterrows():\n",
        "        case_when_string = f\"\"\"WHEN ep.value.string_value LIKE \"{row['page_title']}\" THEN '{row['key']}'\\n\"\"\"\n",
        "        case_when_string_list.append(case_when_string)\n",
        "    return (\" \".join(select_key_string_list), \" \".join(case_when_string_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c-Ygz21SHCwb",
      "metadata": {
        "id": "c-Ygz21SHCwb"
      },
      "outputs": [],
      "source": [
        "(select_key_string, case_when_string) = get_query_string(list(df[\"key\"]), df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AZdJV8IZyvcd",
      "metadata": {
        "id": "AZdJV8IZyvcd"
      },
      "outputs": [],
      "source": [
        "print(select_key_string)\n",
        "# dynamic select query based on user selected pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85nXoYT4yz2o",
      "metadata": {
        "id": "85nXoYT4yz2o"
      },
      "outputs": [],
      "source": [
        "print(case_when_string)\n",
        "# dynamic case when query based on user selected pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3_nlQwaVS--2",
      "metadata": {
        "id": "3_nlQwaVS--2"
      },
      "outputs": [],
      "source": [
        "rl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nnfl8VImHHm0",
      "metadata": {
        "id": "nnfl8VImHHm0"
      },
      "outputs": [],
      "source": [
        "Hey Carlosquery = f\"\"\"\n",
        "CREATE OR REPLACE TABLE\n",
        "  ga4_ecomm_feature_set.ga4_features AS\n",
        "SELECT\n",
        "  item_page_view_table.user_pseudo_id,\n",
        "  {select_key_string}\n",
        "  total_event_table.total_event_count,\n",
        "  IFNULL(c.count_add_to_cart_count,\n",
        "    0) AS count_add_to_cart,\n",
        "  IFNULL(d.count_purchase_count,\n",
        "    0) AS count_purchase,\n",
        "  IFNULL(browse_feature.favorite_device_medium,\n",
        "    'NotAvailable') AS favorite_device_medium,\n",
        "  IFNULL(browse_feature.favorite_mobile_company_name,\n",
        "    'NotAvailable') AS favorite_mobile_company_name,\n",
        "  IFNULL(browse_feature.favorite_lang,\n",
        "    'NotAvailable') AS favorite_lang,\n",
        "  IFNULL(browse_feature.most_used_country,\n",
        "    'NotAvailable') AS most_used_country,\n",
        "  IFNULL(browse_feature.most_used_campaign,\n",
        "    'NotAvailable') AS most_used_campaign,\n",
        "  IFNULL(browse_feature.average_engagement_time_minute,\n",
        "    0) AS average_engagement_time_minute,\n",
        "FROM (\n",
        "  SELECT\n",
        "    *\n",
        "  FROM (\n",
        "    SELECT\n",
        "      user_pseudo_id,\n",
        "      CASE {case_when_string}\n",
        "      ELSE\n",
        "      \"Others\"\n",
        "    END\n",
        "      AS page_title,\n",
        "      COUNT(ep.value.string_value) AS pageVisit_count\n",
        "    FROM\n",
        "      `{PROJECT_ID_DATA}.{DATASET_ID_DATA}.events_*`,\n",
        "      UNNEST(event_params) AS ep\n",
        "    WHERE\n",
        "      ep.key = 'page_title'\n",
        "      AND ep.value.string_value IN {tuple(df['page_title'])}\n",
        "      AND event_date BETWEEN '{START_DATE}'\n",
        "      AND '{END_DATE}'\n",
        "      AND event_name = 'page_view'\n",
        "    GROUP BY\n",
        "      user_pseudo_id,\n",
        "      page_title ) PIVOT ( SUM(pageVisit_count) AS pageVisit_count_sum FOR page_title IN {tuple(df['key'])} ) ) item_page_view_table\n",
        "LEFT JOIN (\n",
        "  SELECT\n",
        "    user_pseudo_id,\n",
        "    COUNT(DISTINCT(event_date)) AS total_event_count,\n",
        "  FROM\n",
        "    `{PROJECT_ID_DATA}.{DATASET_ID_DATA}.events_*`\n",
        "  WHERE\n",
        "    event_date BETWEEN '{START_DATE}'\n",
        "    AND '{END_DATE}'\n",
        "  GROUP BY\n",
        "    user_pseudo_id) total_event_table\n",
        "ON\n",
        "  item_page_view_table.user_pseudo_id = total_event_table.user_pseudo_id\n",
        "LEFT JOIN (\n",
        "    SELECT\n",
        "        user_pseudo_id,\n",
        "        COUNT(*) AS count_add_to_cart_count\n",
        "      FROM\n",
        "        `{PROJECT_ID_DATA}.{DATASET_ID_DATA}.events_*`,\n",
        "        UNNEST (event_params) AS ep\n",
        "      WHERE\n",
        "        event_name = 'add_to_cart'\n",
        "        AND\n",
        "        event_date BETWEEN '{START_DATE}'\n",
        "    AND '{END_DATE}'\n",
        "      GROUP BY\n",
        "        user_pseudo_id) c\n",
        "    ON\n",
        "      item_page_view_table.user_pseudo_id = c.user_pseudo_id\n",
        "LEFT JOIN (\n",
        "    SELECT\n",
        "        user_pseudo_id,\n",
        "        COUNT(*) AS count_purchase_count\n",
        "      FROM\n",
        "        `{PROJECT_ID_DATA}.{DATASET_ID_DATA}.events_*`,\n",
        "        UNNEST (event_params) AS ep\n",
        "      WHERE\n",
        "        event_name = 'purchase'\n",
        "        AND\n",
        "        event_date BETWEEN '{START_DATE}'\n",
        "    AND '{END_DATE}'\n",
        "      GROUP BY\n",
        "        user_pseudo_id) d\n",
        "    ON\n",
        "      item_page_view_table.user_pseudo_id = d.user_pseudo_id\n",
        "LEFT JOIN (\n",
        "  SELECT\n",
        "    user_pseudo_id,\n",
        "    MAX(device.category) AS favorite_device_medium,\n",
        "    MAX(device.mobile_brand_name) AS favorite_mobile_company_name,\n",
        "    MAX(device.LANGUAGE) AS favorite_lang,\n",
        "    MAX(geo.country) AS most_used_country,\n",
        "    MAX(\n",
        "    IF\n",
        "      (events.key = \"campaign\",\n",
        "        events.value.string_value,\n",
        "        NULL)) AS most_used_campaign,\n",
        "    ROUND(AVG(\n",
        "      IF\n",
        "        (events.key = \"engagement_time_msec\",\n",
        "          events.value.int_value,\n",
        "          NULL))/60000,2) AS average_engagement_time_minute,\n",
        "  FROM\n",
        "    `{PROJECT_ID_DATA}.{DATASET_ID_DATA}.events_*`,\n",
        "    UNNEST(event_params) AS events,\n",
        "    UNNEST(items) AS item\n",
        "  WHERE\n",
        "    event_date BETWEEN '{START_DATE}'\n",
        "    AND '{END_DATE}'\n",
        "  GROUP BY\n",
        "    user_pseudo_id ) browse_feature\n",
        "ON\n",
        "  item_page_view_table.user_pseudo_id = browse_feature.user_pseudo_id\n",
        "WHERE\n",
        "  item_page_view_table.user_pseudo_id IS NOT NULL\n",
        "\"\"\"\n",
        "query_job = client.query(query)\n",
        "# print(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EsYLSb48KIHP",
      "metadata": {
        "id": "EsYLSb48KIHP"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  `ga4_ecomm_feature_set.ga4_features`\n",
        "LIMIT\n",
        "5\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "result_df = query_job.to_dataframe()\n",
        "result_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I4i8g9LDc5RA",
      "metadata": {
        "id": "I4i8g9LDc5RA"
      },
      "source": [
        "## BQML Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ma6oHvBxjkXc",
      "metadata": {
        "id": "Ma6oHvBxjkXc"
      },
      "source": [
        "Now that you have the feature data stored in 'ga4_features' table, you can start the BQML K-means Modeling.\n",
        "\n",
        "You should define dataset_id and feature_table, which can be used while building Python code for modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EXcfeCpSAaOL",
      "metadata": {
        "id": "EXcfeCpSAaOL"
      },
      "outputs": [],
      "source": [
        "dataset_id = \"ga4_ecomm_feature_set\"\n",
        "feature_table = \"ga4_features\"  # table name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nWHRx9Nzi6kw",
      "metadata": {
        "id": "nWHRx9Nzi6kw"
      },
      "source": [
        "Before starting the model, there are some important points to remember:\n",
        "\n",
        "1) Remove user_pseudo_id, as it is not a feature but an identifier for the user. In addition, this column has high cardinality, meaning a higher amount of unique values, and hence should not be taken during model building. Make sure to remove any such columns (typical examples like - serial. nos, some IDs). Otherwise, model building times will be massive and might not yield good results.\n",
        "\n",
        "2) The dataset contains both categorical and numerical columns, and BQML will take care of normalizations and categorical encoding automcatically.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The most important part of K-means clustering is to decide the number of clusters to be used during model training. The parameter `num_cluster` has to be determined by user. The value chooses how many segments you want to do in your data. Depending on business requirements, it can range from values as low as 3 and can go up to 100 or more.\n",
        "\n",
        "Some basic examples like hand-written digits clustering will have a straightforward `num_clusters` value as 10, since it's known beforehand.\n",
        "\n",
        "Similarly, maybe your marketing or business teams would like to run some campaigns against three segments of users. Here, you can also observe that `num_clusters` value is know (k=3).\n",
        "\n",
        "However, it is not always this straightforward; hence, identifying the correct value for such situations is crucial.\n",
        "\n",
        "For such situations, we can use the [Hyper Parameter Tuning](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-hyperparameter-tuning) feature of BQML to identify the best value of `num_clusters` based on the [Davis-Bouldin](https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index) score and elbow method.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ICB3F0Cfk7Lq",
      "metadata": {
        "id": "ICB3F0Cfk7Lq"
      },
      "source": [
        "---\n",
        "\n",
        "For this experiment, it is important to set a few parameters:\n",
        "- min_cluster_num - minimum number for clusters\n",
        "\n",
        "- max_cluster_num - maximum number for clusters\n",
        "\n",
        "- num_trails - The maximum number of submodels to train. The tuning will stop when num_trials submodels are trained or when the hyperparameter search space is exhausted. The maximum value is 100. Since we have fourteen values to explore, we will set this as 14.\n",
        "\n",
        "- max_parallel_trails - The maximum number of trials to run at the same time. The default value is one, and the maximum value is 5. We will set this at five the max since there are a lot of parameters to be explored, and hence parallelizing them would make the process faster.\n",
        "\n",
        "\n",
        "This query will take some time to run, roughly around ~8 minutes for this data. It may vary for your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZDybb73d3Ht5",
      "metadata": {
        "id": "ZDybb73d3Ht5"
      },
      "outputs": [],
      "source": [
        "min_cluster_num = 4\n",
        "max_cluster_num = 20\n",
        "model_name = \"model_hptune\"\n",
        "\n",
        "model_hp_tunning_query = f\"\"\"\n",
        "CREATE MODEL\n",
        "  `ga4_ecomm_feature_set.{model_name}`\n",
        "OPTIONS\n",
        "  ( MODEL_TYPE='KMEANS',\n",
        "    NUM_CLUSTERS = hparam_range({min_cluster_num},{max_cluster_num}),\n",
        "    num_trials=14,\n",
        "    max_parallel_trials=5\n",
        "    ) AS\n",
        "SELECT\n",
        "  * EXCEPT (user_pseudo_id)\n",
        "FROM `{PROJECT_ID}.{dataset_id}.{feature_table}`\n",
        "\n",
        "\"\"\"\n",
        "query_job = client.query(model_hp_tunning_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EcWluRULlzyO",
      "metadata": {
        "id": "EcWluRULlzyO"
      },
      "source": [
        "Once the experiment finishes (around ~ 8 minutes), you can extract the Davies Bouldin score using ML.TRAIL_INFO method. These values will be required to identify the best deal.\n",
        "Ideally, the lowest value of the score gives the best value of `num_clusters`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vsLmj--gDnZ9",
      "metadata": {
        "id": "vsLmj--gDnZ9"
      },
      "outputs": [],
      "source": [
        "ml_evaluate_query = f\"\"\"\n",
        "WITH trial_data as (\n",
        "select *\n",
        "    FROM ML.TRIAL_INFO(MODEL ga4_ecomm_feature_set.{model_name})\n",
        ")\n",
        "select\n",
        "trial_id,\n",
        "hyperparameters.NUM_CLUSTERS,\n",
        "hparam_tuning_evaluation_metrics.davies_bouldin_index\n",
        "from\n",
        "trial_data\n",
        "\"\"\"\n",
        "query_job = client.query(ml_evaluate_query)\n",
        "ml_info_df = query_job.to_dataframe()\n",
        "ml_info_df.sort_values(by=\"NUM_CLUSTERS\", inplace=True)\n",
        "ml_info_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "puXGcHmEmXwW",
      "metadata": {
        "id": "puXGcHmEmXwW"
      },
      "source": [
        "However, the lowest value is not always the best indicator. The best value is the value after which the score stabilizes and doesn't drop or increase much. This can be easily figured out using an elbow curve by plotting a line graph of score vs. cluster values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ORQPIygemhWZ",
      "metadata": {
        "id": "ORQPIygemhWZ"
      },
      "source": [
        "![](https://media.geeksforgeeks.org/wp-content/uploads/20190606105550/distortion1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90WGL_JKnmbi",
      "metadata": {
        "id": "90WGL_JKnmbi"
      },
      "source": [
        "In the above example, you can see that we are plotting distortions (David Bouldin Score) vs. K value (`num_clusters`). The goal is to find the \"elbow\" position of the graph and select that value for K.\n",
        "\n",
        "In the grapgh, you can see, K=3 or 4 seems to be an \"elbow,\" after which not much reduction in the score is observed, and it stabilizes. This is just an example; now you can draw a similar graph for the trials we have just performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xk5vKp6DD2Iv",
      "metadata": {
        "id": "Xk5vKp6DD2Iv"
      },
      "outputs": [],
      "source": [
        "fig = px.line(ml_info_df, x=\"NUM_CLUSTERS\", y=[\"davies_bouldin_index\"], markers=True)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aZuNUZtvp3fi",
      "metadata": {
        "id": "aZuNUZtvp3fi"
      },
      "source": [
        "In the graph, you don't get a proper elbow which can mean three things:\n",
        "\n",
        "1) You need more or better features to get better clusters, or\n",
        "\n",
        "2) You need to increase the `num_clusters` values in the trial.\n",
        "\n",
        "3) Data inherently doesn't have clusters and can not be segmented.\n",
        "\n",
        "You can try out the first two methods. However, be very careful in increasing the K value from 20 since it should have a strong business reason.\n",
        "\n",
        "Usually, the more the K value, the harder it is to make sense of the underlying features of clusters since there will be a lot of overlaps between values.\n",
        "\n",
        "In the graph, you can notice that the lowest value of David Bouldin's score is sitting at k=20. However, analyzing those many clusters would be challenging and overlapping, so for simplicity, we keep that at k=5 and assume that data has inherent clusters.\n",
        "\n",
        "As discussed earlier, this should be in tandem with your business objectives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wYb1WZdFJh_4",
      "metadata": {
        "id": "wYb1WZdFJh_4"
      },
      "outputs": [],
      "source": [
        "num_cluster = 5\n",
        "model_name = \"model_c5\"\n",
        "\n",
        "model_query = f\"\"\"\n",
        "CREATE OR REPLACE MODEL\n",
        "  `ga4_ecomm_feature_set.{model_name}`\n",
        "OPTIONS\n",
        "  ( MODEL_TYPE='KMEANS',\n",
        "    NUM_CLUSTERS = {num_cluster},\n",
        "    KMEANS_INIT_METHOD='KMEANS++'\n",
        "    ) AS\n",
        "SELECT\n",
        "  * EXCEPT (user_pseudo_id)\n",
        "FROM `{PROJECT_ID}.{dataset_id}.{feature_table}`\n",
        "\"\"\"\n",
        "query_job = client.query(model_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R8r-_8E0ESDA",
      "metadata": {
        "id": "R8r-_8E0ESDA"
      },
      "source": [
        "Once the training is complete, model objects are stored in BQ. You can look into the model evaluations through the BQ console that gives you cluster statistics.\n",
        "\n",
        "For example, the below images split the evaluations into numeric and categorical columns."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98pxCQ4WKMz_",
      "metadata": {
        "id": "98pxCQ4WKMz_"
      },
      "source": [
        "![](https://screenshot.googleplex.com/bkMq8m5bvDJVzQL.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yJO5I8G3EhP4",
      "metadata": {
        "id": "yJO5I8G3EhP4"
      },
      "source": [
        "From the above table, you can observe the total count of records in each cluster and the centroid values (approximately average) of each numerical feature shown in the green bar with value. The values can give you a good insight into the character of each cluster, for example:\n",
        "\n",
        "- Cluster 1: Group of customers interested in Google Dino Game Tshirts and Youtube Merchandise. The marketing/sales team can use this information later to target these specific customers for any new game t-shirts or YouTube inventory.\n",
        "\n",
        "\n",
        "This, however, is just an evaluation of the training data. In actual scenarios, you will use this model to predict clusters on new data and then analyze each character in detail."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KLoBf50gK1AT",
      "metadata": {
        "id": "KLoBf50gK1AT"
      },
      "source": [
        "![](https://screenshot.googleplex.com/CFqy9ezurZPhP7o.png)\n",
        "\n",
        "![](https://screenshot.googleplex.com/BDVD8ASxAPkFCLL.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GyT5XFRoHgxc",
      "metadata": {
        "id": "GyT5XFRoHgxc"
      },
      "source": [
        "Similarly above graph is a summary of statistics for categorical data. It presents the count distribution for each category for a given categorical column.\n",
        "\n",
        "In the example, you can see the graph for two columns, `favorite_mobile_company_name` and `most_used_campaign.`\n",
        "This can also help you understand some specific characters of each cluster.\n",
        "\n",
        "For example, in the second graph for `most_used_campign,` cluster 4 is a dominant \"referral\" customers.\n",
        "\n",
        "We will explore detailed exploration and predictions in the next section.\n",
        "\n",
        "The point to note here is that these are not the best clusters. You can already see overlapping values among features and uneven distributions of the count of users. You can again refer to the David Bouldin Score graph and notice that at k=5, we are not at the lowest loss (score).\n",
        "\n",
        "However, we chose to pick five to keep the analysis simple. In ideal scenarios, you would coordinate with the business and try to balance choosing k values with your score."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nvoChYnuc_Af",
      "metadata": {
        "id": "nvoChYnuc_Af"
      },
      "source": [
        "## Batch Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1laQchX5LDkW",
      "metadata": {
        "id": "1laQchX5LDkW"
      },
      "source": [
        "Now that the model is finalized, it's time to make batch predictions on new or incoming data. Since we don't have either, we will try to predict the same data (features) we used for modeling.\n",
        "\n",
        "The most crucial point is that the model prediction will happen on features and not on raw data in ideal scenarios. So, when new or incoming data comes, you need to run the feature engineering section to generate new features that your model can accept for batch predictions.\n",
        "\n",
        "For simplification and demonstration purposes, we use the same features for predictions used to train the model.\n",
        "\n",
        "You can use the `ML.PREDICT` method on the features. Here you don't need to exclude user_pseduo_id since that will be required to retarget customers based on their clusters ids.\n",
        "\n",
        "Another essential thing to do is to save the outputs in a BQ table so that it can be later used for any further integrations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nxjJ9F9RAa6n",
      "metadata": {
        "id": "nxjJ9F9RAa6n"
      },
      "outputs": [],
      "source": [
        "prediction_data_table_name = \"model_prediction_c5\"\n",
        "query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE\n",
        "  ga4_ecomm_feature_set.{prediction_data_table_name} AS\n",
        "SELECT\n",
        "  * EXCEPT(NEAREST_CENTROIDS_DISTANCE)\n",
        "FROM\n",
        "  ML.PREDICT(MODEL ga4_ecomm_feature_set.{model_name},\n",
        "    (\n",
        "    SELECT\n",
        "      *\n",
        "    FROM\n",
        "      `{PROJECT_ID}.{dataset_id}.{feature_table}`))\n",
        "\"\"\"\n",
        "query_job = client.query(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mz8_PxJnOPxy",
      "metadata": {
        "id": "mz8_PxJnOPxy"
      },
      "outputs": [],
      "source": [
        "query = f\"\"\"\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  {PROJECT_ID}.ga4_ecomm_feature_set.{prediction_data_table_name}\n",
        "LIMIT 5\n",
        "\"\"\"\n",
        "query_job = client.query(query)\n",
        "predict_data = query_job.to_dataframe()\n",
        "predict_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0XrobniqOVGB",
      "metadata": {
        "id": "0XrobniqOVGB"
      },
      "source": [
        "You can see the table with predictions has a column `CENTROID_ID` that contains their cluster assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bgUVx0uxASZd",
      "metadata": {
        "id": "bgUVx0uxASZd"
      },
      "source": [
        "## Prediction Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D2IkHZiroFEj",
      "metadata": {
        "id": "D2IkHZiroFEj"
      },
      "source": [
        "The last step would be to analyze and understand each cluster in more detail. This analysis will help customize triggers to Google Ads Connector later.\n",
        "\n",
        "This step is also crucial since it helps validate the segmentation modeling from a business point of view and helps pivot if necessary.\n",
        "\n",
        "To achieve the cluster-specific behavior, you can do a `group by` `CENTROID_ID` and get the weights' average.\n",
        "\n",
        "The average weight signifies their affinity to \"visit\" or \"browse\" specific pages they have visited. This value can help identify clusters of people interested in some particular types of pages, as one of the clusters could be everyone who likes \"Hats,\" etc.\n",
        "\n",
        "\n",
        "For simplification, we are only trying to segment customers based on their affinity to visit some specific pages like \"hats,\" \"apparel,\" \"drinkware,\" etc.\n",
        "\n",
        "You can and should include more features based on your business requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K8suehp0c3Xf",
      "metadata": {
        "id": "K8suehp0c3Xf"
      },
      "outputs": [],
      "source": [
        "query = f\"\"\"\n",
        "SELECT\n",
        "  CENTROID_ID,\n",
        "  COUNT(user_pseudo_id) as total_user,\n",
        "  AVG(Apparel_visit_count) as avg_apparel_weight,\n",
        "  AVG(MensUnisex_visit_count) as avg_unisex_weight,\n",
        "  AVG(Drinkware_visit_count) as avg_drinkware_weight,\n",
        "  AVG(Bags_visit_count) as avg_bags_weight,\n",
        "  AVG(Hats_visit_count) as avg_hats_weight,\n",
        "  AVG(Womens_visit_count) as avg_womens_weight,\n",
        "  AVG(CampusCollection_visit_count) as avg_campus_weight,\n",
        "  AVG(Sale_visit_count) as avg_salepage_weight,\n",
        "  AVG(YouTube_visit_count) as avg_youtubepage_weight,\n",
        "  AVG(average_engagement_time_minute) as avg_timespent\n",
        "FROM\n",
        "  `{PROJECT_ID}.{dataset_id}.{prediction_data_table_name}`\n",
        "GROUP BY CENTROID_ID\n",
        "ORDER BY CENTROID_ID\n",
        "\"\"\"\n",
        "query_job = client.query(query)\n",
        "predict_data = query_job.to_dataframe()\n",
        "predict_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FbfGcPMSoN80",
      "metadata": {
        "id": "FbfGcPMSoN80"
      },
      "source": [
        "Once you have the average values for each feature (numerical), you can find the most dominant feature where the values are high.\n",
        "\n",
        "For that, you can take the top 2 values and their feature names for each cluster. You can also write a dynamic python function that takes all the rows and then sorts them to take the topmost values based on N (2 in our case, since we need the top 2 high values).\n",
        "\n",
        "The N is dominanat_category in the code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1t3TEqriTMls",
      "metadata": {
        "id": "1t3TEqriTMls"
      },
      "outputs": [],
      "source": [
        "dominant_category = 2\n",
        "\n",
        "\n",
        "def check_max(row):\n",
        "    new_row = row.drop([\"total_user\", \"CENTROID_ID\", \"avg_timespent\"])\n",
        "    return list(new_row.sort_values(ascending=False).index)[:dominant_category]\n",
        "\n",
        "\n",
        "predict_data[\"dominant_categories\"] = predict_data.apply(check_max, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ApWUb_4lVqcr",
      "metadata": {
        "id": "ApWUb_4lVqcr"
      },
      "outputs": [],
      "source": [
        "predict_data[[\"CENTROID_ID\", \"total_user\", \"dominant_categories\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4DUw49sosUz",
      "metadata": {
        "id": "d4DUw49sosUz"
      },
      "source": [
        "This will give each cluster the most dominant features (based on their highest average weight of page viewing) and total_user in each cluster. Now that you have that, you can explain the characteristics property of each cluster.\n",
        "\n",
        "- Cluster 1 -> Customers who love bags and are interested in unisex collections.\n",
        "- Cluster 2 -> Customers who love hats and apparel.\n",
        "- Cluster 3 -> Customers who love hats but also love unisex collections. This is similar to cluster 2 since both customers love hats. You can combine both clusters (2 & 3) and call customers who love hats.\n",
        "- Cluster 4 -> Customers who are only interested in unisex collections, and usually visit the sales page. They can be targeted if a sale goes in a unisex collection.\n",
        "- Cluster 5 -> Customers who are interested in the general apparel section and love the YouTube merchandise. They can be targeted for any new apparel launches or some new merchandise for YouTube.\n",
        "\n",
        "\n",
        "These overly simplified characteristics assume other features do not affect their browsing behavior. Sometimes, simplification is essential to make the single characteristics stand out and make targeting easier.\n",
        "\n",
        "This is not 100% correct segments, but they are approximately correct. The goal is not to find 100% accurate clusters but to find some approximation of their behaviors such that some Google Ads targeting can be done in dynamic ways.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ArrMQ5Pfn9ue",
      "metadata": {
        "id": "ArrMQ5Pfn9ue"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "You can also plot the weights in Heat Maps. Heats maps are easier to identify the dominant features because of the value legend on the right.\n",
        "\n",
        "For example, Cluster 2 can be seen as dominant with \"hats\" and \"apparel.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6RNk4aEoc7UN",
      "metadata": {
        "id": "6RNk4aEoc7UN"
      },
      "outputs": [],
      "source": [
        "fig = px.imshow(predict_data.drop(\"total_user\", axis=1), text_auto=True)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MneZoCsCAjeT",
      "metadata": {
        "id": "MneZoCsCAjeT"
      },
      "source": [
        "## Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GrlLtarQn2a4",
      "metadata": {
        "id": "GrlLtarQn2a4"
      },
      "source": [
        "Once everything is finalized and your business has given the green light on cluster characteristics, it's time to integrate this further into Google Ads Connector.\n",
        "\n",
        "The GAC (commonly referred to as Tentacles) can send a massive amount of data to GMP (e.g., Google Analytics, Campaign Manager) or Google Ads automatically and reliably.\n",
        "\n",
        "In the next part of the series in the GA4 patterns, you will see how to achieve the integration.\n",
        "\n",
        "First, you will learn to set up Tentacles and then connect it to BQ table where we have the final centroids for all `user_pseudo_ids`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdeJo1Q7d-xY",
      "metadata": {
        "id": "bdeJo1Q7d-xY"
      },
      "source": [
        "## Clean Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vFSfNFYMeFIN",
      "metadata": {
        "id": "vFSfNFYMeFIN"
      },
      "outputs": [],
      "source": [
        "# # Are you sure you want to do this? This is to delete all models\n",
        "# models = client.list_models(dataset_id) # Make an API request.\n",
        "# for model in models:\n",
        "#     full_model_id = f\"{model.dataset_id}.{model.model_id}\"\n",
        "#     client.delete_model(full_model_id)  # Make an API request.\n",
        "#     print(f\"Deleted: {full_model_id}\")\n",
        "# # Are you sure you want to do this? This is to delete all tables and views\n",
        "# tables = client.list_tables(dataset_id)  # Make an API request.\n",
        "# for table in tables:\n",
        "#     full_table_id = f\"{table.dataset_id}.{table.table_id}\"\n",
        "#     client.delete_table(full_table_id)  # Make an API request.\n",
        "#     print(f\"Deleted: {full_table_id}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MneZoCsCAjeT"
      ],
      "name": "GA4_Audience_Segmentation_Notebook_and_Implementation_Guide.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
