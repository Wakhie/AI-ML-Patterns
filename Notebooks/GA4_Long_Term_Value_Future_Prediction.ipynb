{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tpwX9ySYHqwm",
      "metadata": {
        "id": "tpwX9ySYHqwm"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# Author : Lavi Nigam, ML Engineering @ Google\n",
        "# Linkedin: https://www.linkedin.com/in/lavinigam/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aBoKd8UlHuMk",
      "metadata": {
        "id": "aBoKd8UlHuMk"
      },
      "source": [
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0462bd41-ac34-4250-9307-3843e25f2a87",
      "metadata": {
        "id": "0462bd41-ac34-4250-9307-3843e25f2a87"
      },
      "source": [
        "## E-Commerce Future Life Time Value (LTV) prediction using using Google Analytics (GA4) Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1bca8e3-ea6f-4a68-8358-659ff9fc6df0",
      "metadata": {
        "id": "f1bca8e3-ea6f-4a68-8358-659ff9fc6df0"
      },
      "source": [
        "Learn how to build a system to predict Long Term Value of a GA4 e-commerce user using BigQuery ML (BQML)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YAp7O3eu5iz_",
      "metadata": {
        "id": "YAp7O3eu5iz_"
      },
      "source": [
        "Client Lifetime Value, also known as Lifetime Value (LTV) and Customer Lifetime Value (CLV), is an important measure that is used in marketing to indicate an estimate of the net profit that would result from an entire future connection with a customer.\n",
        "\n",
        "Many marketers make an effort to target specific individuals or groups of users who are quite similar to one another with their adverts, but they do not always promote to their most valued consumers. In the context of business, the Pareto principle, which states that 20 percent of a company's clients are responsible for 80 percent of its revenue, is frequently invoked.\n",
        "\n",
        "What would happen if you were able to determine which of your clients will constitute that 20% of your business not just in the past but also in the future? Those consumers may be located through the process of predicting the customer lifetime value, also known as CLV.\n",
        "\n",
        "\n",
        "CLV models allow you to find answers to the following sorts of inquiries regarding customers:\n",
        "\n",
        "* Number of purchases: During a certain period of time in the future, how many separate purchases does the client anticipate making?\n",
        "* Lifetime: The amount of time that will elapse before the client is rendered completely inactive forever.\n",
        "\n",
        "* Value in terms of currency: How much value in terms of currency will the client create over a specific period of time in the future?\n",
        "\n",
        "There are two main difficulties to consider when attempting to forecast future lifetime value, each of which calls for its own unique set of data and modelling strategies:\n",
        "\n",
        "* Determine the future worth of existing consumers who have a known transaction history and make your predictions using that information.\n",
        "\n",
        "* Make an educated guess about the future worth of new consumers who have recently made their initial purchase."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16051aa1-62b0-44a5-83ae-dd038f849eee",
      "metadata": {
        "id": "16051aa1-62b0-44a5-83ae-dd038f849eee"
      },
      "source": [
        "With recent changes, BigQuery ML can directly access GA4 data, bringing capture app and web data in a single interface. This integration opens many opportunities for various machine learning use cases and potential customers. For example, the e-commerce industry can funnel their GA4 data to BQML and expand their analytics with ML capabilities. This pattern aims to help such companies leverage different ML algorithms and scale their analytics with BQ."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lwYLPV4xUuAC",
      "metadata": {
        "id": "lwYLPV4xUuAC"
      },
      "source": [
        "## Audience\n",
        "The pattern is intended for marketing analytics teams for an enterprise, or, teams explicitly responsible for analyzing Google Analytics data. It assumes that you have basic knowledge of the following:\n",
        "\n",
        "- Machine Learning concepts\n",
        "- Standard SQL & Python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9150b5b9-b8a0-443e-ac34-77033acec59a",
      "metadata": {
        "id": "9150b5b9-b8a0-443e-ac34-77033acec59a"
      },
      "source": [
        "## Costs\n",
        "This tutorial uses the following billable components of Google Cloud:\n",
        "\n",
        "- BigQuery\n",
        "- BigQuery ML\n",
        "- Cloud Storage\n",
        "\n",
        "\n",
        "To generate a cost estimate based on your projected usage, use the pricing calculator.\n",
        "\n",
        "Learn about\n",
        "- [BigQuery\n",
        "pricing](https://cloud.google.com/bigquery/pricing),\n",
        "- [BigQuery ML pricing](https://cloud.google.com/bigquery-ml/pricing),\n",
        "- [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing),\n",
        "\n",
        "and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nSxUki7YZjJt",
      "metadata": {
        "id": "nSxUki7YZjJt"
      },
      "source": [
        "## The Dataset\n",
        "The solution uses the public [GA4 Google Merchandise Store](https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=ga4_obfuscated_sample_ecommerce) dataset.\n",
        "\n",
        "Google Merchandise Store is an online store that sells Google-branded merchandise. The site uses Google Analytics 4's standard web ecommerce implementation along with enhanced measurement. The ga4_obfuscated_sample_ecommerce dataset available through the BigQuery Public Datasets program and contains a sample of obfuscated BigQuery event export data for three months from 2020-11-01 to 2021-01-31.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jx5ZSKnvw5fO",
      "metadata": {
        "id": "jx5ZSKnvw5fO"
      },
      "source": [
        "This dataset contains obfuscated data that emulates what a real world dataset would look like from an actual Google Analytics 4 implementation. Certain fields will contain placeholder values including <Other>, NULL, and \" \" . Due to obfuscation, internal consistency of the dataset might be somewhat limited.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Du_xhjb2w8UF",
      "metadata": {
        "id": "Du_xhjb2w8UF"
      },
      "source": [
        "To play with the data on your BQ Console, follow this [quick code](https://developers.google.com/analytics/bigquery/web-ecommerce-demo-dataset#using_the_dataset)\n",
        "\n",
        "You can check the schema details of the dataset [here](https://support.google.com/analytics/answer/7029846#zippy=)\n",
        "\n",
        "There are total 23 columns in the datasets with mixed datatypes, and approximately 4 million rows (each day event is seperate table in the data and total 92 events(tables) are present)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lxvH0lhKdsP4",
      "metadata": {
        "id": "lxvH0lhKdsP4"
      },
      "source": [
        "## Exporting Google Analytics data to BigQuery\n",
        "If instead of the sample data, you want to use your own data from a GA4 property, you can follow the instructions in [(GA4) Set up BigQuery ](https://support.google.com/analytics/answer/9823238#zippy=%2Cin-this-article) Export to export your data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3Ro0D7_NIJ9t",
      "metadata": {
        "id": "3Ro0D7_NIJ9t"
      },
      "source": [
        "### Set up your local development environment\n",
        "\n",
        "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
        "all the requirements to run this notebook. You can skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "naVsQIkoIKqi",
      "metadata": {
        "id": "naVsQIkoIKqi"
      },
      "source": [
        "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
        "You need the following:\n",
        "\n",
        "* The Google Cloud SDK\n",
        "* Python 3\n",
        "* Jupyter notebook running in a virtual environment with Python 3\n",
        "\n",
        "The Google Cloud guide to [Setting up a Python development\n",
        "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
        "installation guide](https://jupyter.org/install) provide detailed instructions\n",
        "for meeting these requirements. The following steps provide a condensed set of\n",
        "instructions:\n",
        "\n",
        "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
        "\n",
        "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
        "\n",
        "1. [Install\n",
        "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
        "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
        "\n",
        "1. To install Jupyter, run `pip3 install jupyter` on the\n",
        "command-line in a terminal shell.\n",
        "\n",
        "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
        "\n",
        "1. Open this notebook in the Jupyter Notebook Dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-kMgGax-IS1F",
      "metadata": {
        "id": "-kMgGax-IS1F"
      },
      "source": [
        "### Install additional packages\n",
        "\n",
        "Install additional package dependencies not installed in your notebook environment, such as {plotly.express, pandas, google.cloud}. Use the latest major GA version of each package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BNG1a0WFIWsR",
      "metadata": {
        "id": "BNG1a0WFIWsR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cmiF7wD2IXbk",
      "metadata": {
        "id": "cmiF7wD2IXbk"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ng8iy7LaIa9q",
      "metadata": {
        "id": "Ng8iy7LaIa9q"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bKoNzVCxIf7O",
      "metadata": {
        "id": "bKoNzVCxIf7O"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EFwFJmXZIgqV",
      "metadata": {
        "id": "EFwFJmXZIgqV"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "\n",
        "3. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "4. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WbtKuolkIo2k",
      "metadata": {
        "id": "WbtKuolkIo2k"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SsjOE5jkIsuU",
      "metadata": {
        "id": "SsjOE5jkIsuU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"\"\n",
        "\n",
        "# Get your Google Cloud project ID from gcloud\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID: \", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S8aHk0AQIxlM",
      "metadata": {
        "id": "S8aHk0AQIxlM"
      },
      "source": [
        "Otherwise, set your project ID here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kbAokgs1Iz_o",
      "metadata": {
        "id": "kbAokgs1Iz_o"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
        "    PROJECT_ID = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vafnp9fDI-Jy",
      "metadata": {
        "id": "vafnp9fDI-Jy"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Google Cloud Notebooks**, your environment is already\n",
        "authenticated. Skip this step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KmPLvdx-JFti",
      "metadata": {
        "id": "KmPLvdx-JFti"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# If on Google Cloud Notebooks, then don't execute this code\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cf783db-023f-41d1-8753-b194ccefc759",
      "metadata": {
        "id": "5cf783db-023f-41d1-8753-b194ccefc759"
      },
      "outputs": [],
      "source": [
        "# Importing some important libraries that will be used during the notebook\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u4AoPtwKSNhm",
      "metadata": {
        "id": "u4AoPtwKSNhm"
      },
      "outputs": [],
      "source": [
        "# Client manages connections to the BigQuery API and helps\n",
        "# bundle configuration (project, credentials) needed for API requests.\n",
        "client = bigquery.Client(PROJECT_ID)\n",
        "\n",
        "# to make sure all columns are displayed while working with dataframe\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.max_colwidth\", 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2_BpasBjb3eG",
      "metadata": {
        "id": "2_BpasBjb3eG"
      },
      "source": [
        "## Assumptions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oMutkEaqhJsJ",
      "metadata": {
        "id": "oMutkEaqhJsJ"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Jj4q_owsvDI8",
      "metadata": {
        "id": "Jj4q_owsvDI8"
      },
      "source": [
        "You can start by defining some essential variables that can change according to your data. It is always better to consider the most recent records from your data as features. For this purpose, you can set the START_DATE and END_DATE based on your data recency.\n",
        "\n",
        "In this case, the date range is set for 3 months."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YI1pDBZMu5hm",
      "metadata": {
        "id": "YI1pDBZMu5hm"
      },
      "outputs": [],
      "source": [
        "# Dataset (GA4 Google Merchandise Store) specific Variable\n",
        "# Change it to your dataset spefic values, if you want to use the code for your data.\n",
        "# We assume table names will be \"events_*\"\n",
        "PROJECT_ID_DATA = \"bigquery-public-data\"\n",
        "DATASET_ID_DATA = \"ga4_obfuscated_sample_ecommerce\"  # ga4-bq-pattern.1crdata.fake_ga4 #ga4_obfuscated_sample_ecommerce\n",
        "START_DATE = \"20201101\"\n",
        "END_DATE = \"20210131\"  # taking 3 months recent data.\n",
        "# In queries, these variables are editable so that you can put your project, dataset, and date,\n",
        "# making it easier for you to make the least amount of changes. Of course, you don't need to change\n",
        "# it for public data. But, for making the queries editable, it made sense to define them here.\n",
        "# You can run the whole notebook (mostly) with your data by changing values here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9613d67-35cd-40c3-b7e4-f08926c7594f",
      "metadata": {
        "id": "b9613d67-35cd-40c3-b7e4-f08926c7594f"
      },
      "source": [
        "You can start the data exploration by returning the first five rows of data.\n",
        "The data has multiple event tables for each day. So, all the tables (events) could be queried by using events* as the wildcard.\n",
        "\n",
        "[GA4 Data Export Schema](https://support.google.com/analytics/answer/7029846#zippy=)\n",
        "\n",
        "Note: BigQuery export, by default, are [date sharded tables](https://cloud.google.com/bigquery/docs/partitioned-tables#dt_partition_shard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35270b17-147e-495f-8839-ffe363db0aa7",
      "metadata": {
        "id": "35270b17-147e-495f-8839-ffe363db0aa7"
      },
      "outputs": [],
      "source": [
        "query = f\"\"\"\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  `{PROJECT_ID_DATA}.{DATASET_ID_DATA}.events*`\n",
        "LIMIT\n",
        "  5\n",
        "\"\"\"\n",
        "query_job = client.query(query)\n",
        "top5_data = query_job.to_dataframe()\n",
        "top5_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e665b736-7c46-4131-a1fa-d050d40e30f4",
      "metadata": {
        "id": "e665b736-7c46-4131-a1fa-d050d40e30f4"
      },
      "source": [
        "The first five rows of data can help you understand the tables' composite structure of data types. For example, you can see numerical, categorical, Arrays, and Struct as data types. Using this information, later, you will be able to write specific `UNNEST` queries for [Arrays](https://cloud.google.com/bigquery/docs/reference/standard-sql/arrays#query_structs_in_an_array) & [Struct](https://cloud.google.com/bigquery/docs/reference/standard-sql/arrays#querying_array-type_fields_in_a_struct)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MXy4Sy-raeEr",
      "metadata": {
        "id": "MXy4Sy-raeEr"
      },
      "source": [
        "By looking at some columns, you can also identify a few essential features like event_date, event_name, user_ltv, device, geo, traffic_source, platform, and items. However, as discussed earlier, you still are not aware of their value distribution, availability, and data types."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qRGBRNZRYg7A",
      "metadata": {
        "id": "qRGBRNZRYg7A"
      },
      "source": [
        "You can check the data types of each column using [INFORMATION_SCHEMA](https://cloud.google.com/bigquery/docs/information-schema-tables) table. It can give you detailed metadata of your columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rgE--AsdTJnd",
      "metadata": {
        "id": "rgE--AsdTJnd"
      },
      "outputs": [],
      "source": [
        "query = f\"\"\"\n",
        "SELECT\n",
        "  DISTINCT(column_name),\n",
        "  data_type\n",
        "FROM\n",
        "  `{PROJECT_ID_DATA}.{DATASET_ID_DATA}.INFORMATION_SCHEMA.COLUMNS`\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "predict_data = query_job.to_dataframe()\n",
        "predict_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G29DgyP4vjgV",
      "metadata": {
        "id": "G29DgyP4vjgV"
      },
      "source": [
        "You can start by understanding overall data by getting a quick summary of the data, namely - total events  (event_count), total users (user_count), total days in the data (day_count), and total registered users of the platform (registered_user_id).\n",
        "This can help you get a sense of the scale of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rYSCfLhr6dUl",
      "metadata": {
        "id": "rYSCfLhr6dUl"
      },
      "outputs": [],
      "source": [
        "query = f\"\"\"\n",
        "\n",
        "SELECT\n",
        "  COUNT(*) AS event_count,\n",
        "  COUNT(DISTINCT user_pseudo_id) AS user_count,\n",
        "  COUNT(DISTINCT event_date) AS day_count,\n",
        "  COUNT(DISTINCT user_id) AS registered_user_id\n",
        "FROM\n",
        "  `{PROJECT_ID_DATA}.{DATASET_ID_DATA}.events*`\n",
        "\"\"\"\n",
        "query_job = client.query(query)\n",
        "top5_data = query_job.to_dataframe()\n",
        "top5_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "trhmmrbiS334",
      "metadata": {
        "id": "trhmmrbiS334"
      },
      "source": [
        "As you can observe, there are roughly 4 million events with close to 270,000 users, stretched along 92 days of activity on the platform.\n",
        "\n",
        "There are no registered users data in the table. The user_pseudo_id is not a \"user_id\"; it is an client ID (cookie ID) for the user. This means that a single user can be represented as multiple pseudo_id in the data.\n",
        "\n",
        "For simplicity, we will assume that all user_pseudo_id are unique and represent a single user.\n",
        "\n",
        "If your data has 'user_id', use that directly, or else you can go ahead and use 'user_psuudo_id'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J4a69In4lqMh",
      "metadata": {
        "id": "J4a69In4lqMh"
      },
      "source": [
        "---\n",
        "Now, you can start by looking into `event_name` distribution.\n",
        "\n",
        "event_name is a significant column in this dataset. It contains all the events triggered as users interact with the Google Merchandise Store like page_view, scroll (scrolling the page), view_item (viewing specific item), etc. You can refer [here](https://developers.google.com/analytics/devguides/collection/ga4/reference/events) for a more detailed meaning of each event_name.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DbiCdEaxMCBY",
      "metadata": {
        "id": "DbiCdEaxMCBY"
      },
      "outputs": [],
      "source": [
        "query = f\"\"\"\n",
        "SELECT\n",
        "  event_name,\n",
        "  COUNT(*) as row_count\n",
        "FROM\n",
        "   `{PROJECT_ID_DATA}.{DATASET_ID_DATA}.events*`\n",
        "GROUP BY\n",
        "  event_name\n",
        "ORDER BY\n",
        "  row_count DESC\n",
        "\"\"\"\n",
        "query_job = client.query(query)\n",
        "result_df = query_job.to_dataframe()\n",
        "fig = px.bar(\n",
        "    result_df, x=\"row_count\", y=\"event_name\", title=\"Event Name Frequency Distribution\"\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m1spMS_ysKE0",
      "metadata": {
        "id": "m1spMS_ysKE0"
      },
      "source": [
        "You can observe a great imbalance in the frequency of different event_name(s). The top five events based on frequency:\n",
        "\n",
        "* page_view - User is viewing a page\n",
        "\n",
        "* user_engagement - Sessions that last 10 seconds or longer\n",
        "\n",
        "* scroll - User scrolling through a page\n",
        "\n",
        "* view_item - some content was shown to the user. You can use this to discover the most popular items.\n",
        "\n",
        "* session_start - User session after the engagement has been initiated.\n",
        "\n",
        "\n",
        "The other events don't have too many records and hence would be a challenge to be considered a feature. However, you can also notice that typical purchase events - \"add_to_cart\", \"begin_checkout\", \"add_shipping_info\", \"add_payment_info\", and \"purchase\" have a tiny amount of records, indicating that this data doesn't contain too many events where a user has bought something.\n",
        "\n",
        "\n",
        "So,`page_view` seems to be the best filter for the column `event_name` since it has the highest records and covers users' general browsing behavior. However, you can still leverage `add_to_cart` and `purchase` value for purchase information by simply counting a user's total events for these event types.\n",
        "\n",
        "Also, remember that the actual key of `page_view` event_name is available in event_params, and their values are in event_params.values.{int/float/string} in nested array format.\n",
        "\n",
        "Data references:\n",
        "\n",
        "[Dimensions & Metrics](https://support.google.com/analytics/topic/11151952?hl=en&ref_topic=9228654)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1E_xbe1sdYkr",
      "metadata": {
        "id": "1E_xbe1sdYkr"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qi7BCwzx644F",
      "metadata": {
        "id": "qi7BCwzx644F"
      },
      "source": [
        "Now that you have done some basic exploration of GA4 data, you can create different features for LTV.\n",
        "\n",
        "However, before doing that, you should create a Dataset in BQ Console named \"ga4_ecomm_feature_set\" inside your project. Then, you can create a table for different kinds of features and store in the dataset.\n",
        "\n",
        "\n",
        "This will help retain the features for later purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LNhYCNZGNoh1",
      "metadata": {
        "id": "LNhYCNZGNoh1"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = \"ga4_ecomm_feature_set_ltv\"\n",
        "feature_table = \"ltv_features\"\n",
        "try:\n",
        "    dataset = client.create_dataset(DATASET_NAME, timeout=30)  # Make an API request.\n",
        "    print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ZMg5HvvN5Av",
      "metadata": {
        "id": "9ZMg5HvvN5Av"
      },
      "source": [
        "Let's start building features that can be leveraged to make models learn user behavior that can help predict future LTV based on given LTV values pre-computed in the dashboard. Our goal is to go over and beyond the default formula and model used in the dashboard for LTV. [Refer here](https://support.google.com/analytics/answer/9947257?hl=en) to learn more about default LTV value."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6uF84Paok5vC",
      "metadata": {
        "id": "6uF84Paok5vC"
      },
      "source": [
        "Since the feature-building query code will be significantly large, so for readability purposes, we are dividing the query into base feature table and final feature groups-bys.\n",
        "The base table will focus on features where core logic is implemented and multiple tables are created. Once they are created, we can make final aggregates and flags based on previously computed tables in the following query block."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l7kEQqkcdYDU",
      "metadata": {
        "id": "l7kEQqkcdYDU"
      },
      "source": [
        "High-Level Features:\n",
        "\n",
        "* engagement -> User Engagement - session_engaged value =  1.\n",
        "\n",
        "* bounces -> User Bounce - session_engaged value = 0.\n",
        "\n",
        "* returning_customers -> returning customers - if a user made multiple purchases (>=2) events on different dates.\n",
        "\n",
        "* non_returning_customers ->  non-returning customers - if a user has done single or no purchase events on all dates.\n",
        "\n",
        "*  repeated_purchase -> repeated purchase flag - total times purchase has been made. If it's >=0, then the actual number else 0.\n",
        "\n",
        "* events_sequence -> Event Sequence - order of event_name based on event_date.\n",
        "\n",
        "* grouped_events_sequence -> grouping all events\n",
        "\n",
        "* user_events_counted -> User Event counted - total events observed and tracked for the user.\n",
        "\n",
        "* pages -> various pages visited as part of page_view event; multiple levels (all links accessed after the landing page) are tracked.\n",
        "landing_page, second_page, exit page [three levels]\n",
        "\n",
        "          - pagepath_level_1\n",
        "          - previous_page_path_level_1\n",
        "          - landing_pagepath_level_1\n",
        "          - second_pagepath_level_1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VhJ3tzXphmCq",
      "metadata": {
        "id": "VhJ3tzXphmCq"
      },
      "outputs": [],
      "source": [
        "base_feature_tabel = f\"\"\"CREATE OR REPLACE TABLE `{DATASET_NAME}.{feature_table}` AS\n",
        "with engagement as (\n",
        "select\n",
        "   user_pseudo_id,\n",
        "   event_date,\n",
        "   SAFE_DIVIDE(\n",
        "       count(distinct case when session_engaged = 1 then concat(user_pseudo_id,session_id) end),\n",
        "       COUNT(DISTINCT session_id)\n",
        "   ) AS engagement_rate,\n",
        "   count(distinct case when session_engaged = 1 then concat(user_pseudo_id,session_id) end) as engaged_sessions,\n",
        "   count(distinct case when session_engaged = 0 then concat(user_pseudo_id,session_id) end) as bounces,\n",
        "   SAFE_DIVIDE(\n",
        "       count(distinct case when session_engaged = 0 then concat(user_pseudo_id,session_id) end),\n",
        "       COUNT(DISTINCT session_id)\n",
        "   ) as bounce_rate,\n",
        "   COUNT(DISTINCT session_id) AS total_sessions,\n",
        "   IFNULL(round(sum(engagement_time_msec)/1000),0) as engagement_time_seconds,\n",
        "from (\n",
        "   select\n",
        "       user_pseudo_id,\n",
        "       event_date,\n",
        "       (select value.int_value from unnest(event_params) where key = 'ga_session_id') as session_id,\n",
        "       IFNULL(max((select value.int_value from unnest(event_params) where key = 'session_engaged')), 0) as session_engaged,\n",
        "       max((select value.int_value from unnest(event_params) where key = 'engagement_time_msec')) as engagement_time_msec\n",
        "   from\n",
        "       `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events*`\n",
        "   WHERE parse_date(\"%Y%m%d\",event_date) >= parse_date(\"%Y-%m-%d\", '2021-01-01')\n",
        "   group by\n",
        "       user_pseudo_id,\n",
        "       event_date,\n",
        "       session_id)\n",
        "   group by user_pseudo_id, event_date),\n",
        "\n",
        "returning_customers as (\n",
        "SELECT user_pseudo_id, event_date, MAX(unique_purchase) as unique_purchase\n",
        "   FROM (\n",
        "       SELECT\n",
        "           user_pseudo_id,\n",
        "           event_date,\n",
        "           RANK() OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC)\n",
        "               AS unique_purchase\n",
        "       FROM\n",
        "           `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events*` AS GA\n",
        "       WHERE event_name = 'purchase'\n",
        "       AND parse_date(\"%Y%m%d\",event_date) >= parse_date(\"%Y-%m-%d\", '2021-01-01')\n",
        "       GROUP BY user_pseudo_id, event_date, event_timestamp\n",
        "   )\n",
        "   WHERE unique_purchase >= 2\n",
        "   GROUP BY user_pseudo_id, event_date\n",
        "),\n",
        "\n",
        "non_returning_customers as (\n",
        "SELECT\n",
        "       user_pseudo_id,\n",
        "       event_date,\n",
        "   FROM `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events*` AS GA\n",
        "   WHERE user_pseudo_id NOT IN (SELECT user_pseudo_id FROM returning_customers)\n",
        "   AND parse_date(\"%Y%m%d\",event_date) >= parse_date(\"%Y-%m-%d\", '2021-01-01')\n",
        "   GROUP BY user_pseudo_id, event_date\n",
        "),\n",
        "\n",
        "combined as (\n",
        " SELECT user_pseudo_id, event_date, unique_purchase\n",
        "   FROM returning_customers\n",
        "   UNION ALL\n",
        "   SELECT user_pseudo_id, event_date, -1\n",
        "   FROM non_returning_customers\n",
        "   GROUP BY user_pseudo_id, event_date\n",
        "),\n",
        "\n",
        "repeated_purchase as (\n",
        " SELECT\n",
        "   user_pseudo_id,\n",
        "   event_date,\n",
        "   CASE\n",
        "       WHEN unique_purchase >= 0\n",
        "       THEN unique_purchase ELSE 0 END AS has_repeated_purchase\n",
        " FROM\n",
        "   combined\n",
        "),\n",
        "\n",
        "events_sequence AS (\n",
        "   SELECT\n",
        "     ROW_NUMBER() OVER () AS rownumber,\n",
        "     ROW_NUMBER() OVER (PARTITION BY user_pseudo_id ORDER BY event_date) AS rownumber_by_user,\n",
        "     user_pseudo_id,\n",
        "     event_date\n",
        "   FROM (\n",
        "     SELECT DISTINCT\n",
        "           user_pseudo_id,\n",
        "           event_date,\n",
        "       FROM\n",
        "           `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events*` AS GA\n",
        "       WHERE\n",
        "       parse_date(\"%Y%m%d\",event_date) >= parse_date(\"%Y-%m-%d\", '2021-01-01')\n",
        "       GROUP BY user_pseudo_id, event_date\n",
        "   )\n",
        "   ORDER BY user_pseudo_id, event_date\n",
        "),\n",
        "\n",
        "grouped_events_sequence AS (\n",
        " SELECT user_pseudo_id,\n",
        "        event_date,\n",
        "        DENSE_RANK() OVER (ORDER BY rownumber) - DENSE_RANK() OVER (PARTITION BY user_pseudo_id ORDER BY rownumber) AS car_group,\n",
        "        rownumber_by_user\n",
        " FROM events_sequence\n",
        "),\n",
        "\n",
        "user_events_counted AS (\n",
        "SELECT DISTINCT user_pseudo_id,\n",
        "      count_of_consecutive_days,\n",
        "      MAX(count_of_days) as count_of_days\n",
        "FROM\n",
        "(SELECT user_pseudo_id,\n",
        "       COUNT(1) AS count_of_consecutive_days,\n",
        "       MAX(rownumber_by_user) AS count_of_days\n",
        "FROM grouped_events_sequence\n",
        "GROUP BY user_pseudo_id, car_group\n",
        "HAVING COUNT(1) >= 1)\n",
        "GROUP BY user_pseudo_id, count_of_consecutive_days\n",
        "),\n",
        "\n",
        "pages as (\n",
        "select\n",
        "   user_pseudo_id,\n",
        "   (select value.int_value from unnest(event_params) where event_name = 'page_view' and key = 'ga_session_id') as session_id,\n",
        "   event_timestamp,\n",
        "   event_date,\n",
        "   event_name,\n",
        "   (select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location') as page,\n",
        "   lag((select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location'), 1) over (partition by user_pseudo_id,(select value.int_value from unnest(event_params) where event_name = 'page_view' and key = 'ga_session_id') order by event_timestamp asc) as previous_page,\n",
        "   case when split(split((select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location'),'/')[safe_ordinal(4)],'?')[safe_ordinal(1)] = '' then null else concat('/',split(split((select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location'),'/')[safe_ordinal(4)],'?')[safe_ordinal(1)]) end as pagepath_level_1,\n",
        "   case when split(split(lag((select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location'), 1) over (partition by user_pseudo_id,(select value.int_value from unnest(event_params) where event_name = 'page_view' and key = 'ga_session_id') order by event_timestamp asc),'/')[safe_ordinal(4)],'?')[safe_ordinal(1)] = '' then null else concat('/',split(split(lag((select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location'), 1) over (partition by user_pseudo_id,(select value.int_value from unnest(event_params) where event_name = 'page_view' and key = 'ga_session_id') order by event_timestamp asc),'/')[safe_ordinal(4)],'?')[safe_ordinal(1)]) end as previous_page_path_level_1,\n",
        "   (select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_title') as page_title,\n",
        "   case when (select value.int_value from unnest(event_params) where event_name = 'page_view' and key = 'entrances') = 1 then (select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location') end as landing_page,\n",
        "   case when split(split((case when (select value.int_value from unnest(event_params) where event_name = 'page_view' and key = 'entrances') = 1 then (select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location') END),'/')[safe_ordinal(4)],'?')[safe_ordinal(1)] = '' then null else concat('/',split(split((case when (select value.int_value from unnest(event_params) where event_name = 'page_view' and key = 'entrances') = 1 then (select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location') END),'/')[safe_ordinal(4)],'?')[safe_ordinal(1)]) end as landing_pagepath_level_1,\n",
        "   case when (select value.int_value from unnest(event_params) where event_name = 'page_view' and key = 'entrances') = 1 then lead((select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location'), 1) over (partition by user_pseudo_id,(select value.int_value from unnest(event_params) where event_name = 'page_view' and key = 'ga_session_id') order by event_timestamp asc) else null end as second_page,\n",
        "   case when split(split((case when (select value.int_value from unnest(event_params) where event_name = 'page_view' and key = 'entrances') = 1 then lead((select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location'), 1) over (partition by user_pseudo_id,(select value.int_value from unnest(event_params) where event_name = 'page_view' and key = 'ga_session_id') order by event_timestamp asc) else null end),'/')[safe_ordinal(4)],'?')[safe_ordinal(1)] = '' then null else concat('/',split(split((case when (select value.int_value from unnest(event_params) where event_name = 'page_view' and key = 'entrances') = 1 then lead((select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location'), 1) over (partition by user_pseudo_id,(select value.int_value from unnest(event_params) where event_name = 'page_view' and key = 'ga_session_id') order by event_timestamp asc) else null end),'/')[safe_ordinal(4)],'?')[safe_ordinal(1)]) end as second_pagepath_level_1,\n",
        "   case when (select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location') = first_value((select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location')) over (partition by user_pseudo_id,(select value.int_value from unnest(event_params) where event_name = 'page_view' and key = 'ga_session_id') order by event_timestamp desc) then ( select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location') else null end as exit_page,\n",
        "   case when split(split((case when (select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location') = first_value((select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location')) over (partition by user_pseudo_id,(select value.int_value from unnest(event_params) where event_name = 'page_view' and key = 'ga_session_id') order by event_timestamp desc) then ( select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location') else null end),'/')[safe_ordinal(4)],'?')[safe_ordinal(1)] = '' then null else concat('/',split(split((case when (select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location') = first_value((select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location')) over (partition by user_pseudo_id,(select value.int_value from unnest(event_params) where event_name = 'page_view' and key = 'ga_session_id') order by event_timestamp desc) then ( select value.string_value from unnest(event_params) where event_name = 'page_view' and key = 'page_location') else null end),'/')[safe_ordinal(4)],'?')[safe_ordinal(1)]) end as exit_pagepath_level_1,\n",
        "from\n",
        "   -- change this to your google analytics 4 export location in bigquery\n",
        "   `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events*`\n",
        "where\n",
        "   event_name = 'page_view'\n",
        "   AND parse_date(\"%Y%m%d\",event_date) >= parse_date(\"%Y-%m-%d\", '2021-01-01')),\n",
        "\n",
        "pages_tracking as (\n",
        "select\n",
        "   user_pseudo_id,\n",
        "   event_date,\n",
        "   -- page (dimension | a page on the website specified by path and/or query parameters)\n",
        "   page,\n",
        "   -- page path level 1 (dimension | this dimension rolls up all the page paths in the first hierarchical level)\n",
        "   MAX(pagepath_level_1) as pagepath_level_1,\n",
        "   -- previous page path (dimension | a page visited before another page on the same property)\n",
        "   MAX(previous_page) as previous_page,\n",
        "   MAX(previous_page_path_level_1) as previous_page_path_level_1,\n",
        "   -- landing page (dimension | the first page in users' sessions)\n",
        "   landing_page,\n",
        "   MAX(landing_pagepath_level_1) as landing_pagepath_level_1,\n",
        "   -- second page (dimension | the second page in users' sessions)\n",
        "   MAX(second_page) as second_page,\n",
        "   MAX(second_pagepath_level_1) as second_pagepath_level_1,\n",
        "   -- exit page (dimension | the last page in users' sessions)\n",
        "   exit_page,\n",
        "   MAX(exit_pagepath_level_1) as exit_pagepath_level_1,\n",
        "   -- entrances (metric | the number of entrances to the property measured as the first pageview in a session)\n",
        "   count(landing_page) as entrances,\n",
        "   -- pageviews (metric | the total number of pageviews for the property)\n",
        "   count(page) as pageviews,\n",
        "   -- unique pageviews (metric | the number of sessions during which the specified page was viewed at least once, a unique pageview is counted for each page url + page title combination)\n",
        "   count(distinct concat(page,page_title,session_id)) as unique_pageviews,\n",
        "   -- pages / session (metric | the average number of pages viewed during a session, including repeated views of a single page)\n",
        "   count(page) / count(distinct session_id) as pages_per_session,\n",
        "   -- exits (metric | the number of exits from the property)\n",
        "   count(exit_page) as exits,\n",
        "   -- exit % (metric | the percentage of exits from the property that occurred out of the total pageviews)\n",
        "   count(exit_page) / count(page) as exit_rate\n",
        "from\n",
        "   pages\n",
        "group by\n",
        "   user_pseudo_id,\n",
        "   event_date,\n",
        "   page,\n",
        "   page_title,\n",
        "   landing_page,\n",
        "   exit_page\n",
        "),\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0iJyiKBQh18Q",
      "metadata": {
        "id": "0iJyiKBQh18Q"
      },
      "source": [
        "Final  Features (Grouped for user):\n",
        "\n",
        "* Date:  Date specific features -> year,month_of_the_year,week_of_the_year,day_of_the_month,day_of_week,hour.\n",
        "\n",
        "* Count of user event footprints -> count_of_days, count_of_consecutive_days, count_total_events, count_of_sessions, count_view_item, count_add_to_cart_item, count_select_item, count_begin_checkout_item\n",
        "\n",
        "* If the user is visiting for the first time -> is_first_visit\n",
        "\n",
        "* Purchase and Promotion elated -> has_promotionhas_added_payment_infohas_added_shipping_info\n",
        "\n",
        "* Engagement and Bounce numbers based on feature base tables: engagement_rate, bounce_rate, bounces, engagement_time_seconds, total_sessions\n",
        "\n",
        "* Page Trackers based on different most visited pages; pagepath_level_1, previous_page_path_level_1, landing_pagepath_level_1, second_pagepath_level_1\n",
        "\n",
        "* Page view specific numbers: entrances, pageviews, unique_pageviews, pages_per_session, exits, exit_rate, new_or_returning_visitor\n",
        "\n",
        "* The campaign that is used most times and various traffic source: campaign, traffic_medium, traffic_source\n",
        "\n",
        "* The total session engaged; engaged_session_event\n",
        "\n",
        "* GA4 internal model predicted LTV value, which is our class or predictor data; ltv_revenue\n",
        "\n",
        "* The category that is visited most and average item quantity viewed and purchased; category, avg_item_quantity\n",
        "\n",
        "* Device specific information for a user:mobile_brand_name, operating_system, operating_system_version, browser, browser_version\n",
        "\n",
        "* User demographic: continent, sub_continent, country, region, city\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xbFXxKx3h56y",
      "metadata": {
        "id": "xbFXxKx3h56y"
      },
      "outputs": [],
      "source": [
        "grouped_feature_query = \"\"\"master_ga4_erp as (\n",
        "SELECT\n",
        "A.user_pseudo_id,\n",
        "MAX(parse_date(\"%Y%m%d\",A.event_date)) as event_date,\n",
        "MAX(\n",
        " CASE\n",
        " WHEN ecommerce.purchase_revenue > 0\n",
        " THEN 1\n",
        " ELSE 0\n",
        "END) AS has_purchased,\n",
        "MAX(Repeated_Purchased.has_repeated_purchase) as had_purchased_before,\n",
        "MAX(CAST(format_date('%Y',parse_date(\"%Y%m%d\",A.event_date)) as INT64)) as year,\n",
        "MAX(CAST(format_date('%m',parse_date(\"%Y%m%d\",A.event_date)) as INT64)) as month_of_the_year,\n",
        "MAX(CAST(format_date('%U',parse_date(\"%Y%m%d\",A.event_date)) as INT64)) as week_of_the_year,\n",
        "MAX(CAST(format_date('%d',parse_date(\"%Y%m%d\",A.event_date)) as INT64)) as day_of_the_month,\n",
        "MAX(CAST(format_date('%w',parse_date(\"%Y%m%d\",A.event_date)) as INT64)) as day_of_week,\n",
        "MAX(CAST(format(\"%02d\",extract(hour from timestamp_micros(A.event_timestamp))) as INT64)) as hour,\n",
        "MAX(User_Events_Counted.count_of_days) as count_of_days,\n",
        "MAX(User_Events_Counted.count_of_consecutive_days) as count_of_consecutive_days,\n",
        "COUNT(DISTINCT A.event_timestamp) as count_total_events,\n",
        "SUM(CASE WHEN REGEXP_CONTAINS(event_name, '(?i)purchase') THEN 1 ELSE 0 END) as count_item_purchases,\n",
        "SUM(CASE WHEN (REGEXP_CONTAINS(event_name, '(?i)session_start') AND ep.key = 'ga_session_number') THEN  ep.value.int_value ELSE 0 END) as count_of_sessions,\n",
        "SUM(CASE WHEN REGEXP_CONTAINS(event_name, '(?i)view_item') THEN 1 ELSE 0 END) as count_view_item,\n",
        "SUM(CASE WHEN REGEXP_CONTAINS(event_name, '(?i)add_to_cart') THEN 1 ELSE 0 END) as count_add_to_cart_item,\n",
        "SUM(CASE WHEN REGEXP_CONTAINS(event_name, '(?i)select_item') THEN 1 ELSE 0 END) as count_select_item,\n",
        "SUM(CASE WHEN REGEXP_CONTAINS(event_name, '(?i)begin_checkout') THEN 1 ELSE 0 END) as count_begin_checkout_item,\n",
        "SUM(CASE WHEN REGEXP_CONTAINS(event_name, '(?i)first_visit') THEN 1 ELSE 0 END) as is_first_visit,\n",
        "SUM(CASE WHEN REGEXP_CONTAINS(event_name, '(?i)select_promotion') THEN 1 ELSE 0 END) as has_promotion,\n",
        "SUM(CASE WHEN REGEXP_CONTAINS(event_name, '(?i)add_payment_info') THEN 1 ELSE 0 END) as has_added_payment_info,\n",
        "SUM(CASE WHEN REGEXP_CONTAINS(event_name, '(?i)add_shipping_info') THEN 1 ELSE 0 END) as has_added_shipping_info,\n",
        "IFNULL(MAX(Engagement.engagement_rate), 0) AS engagement_rate,\n",
        "IFNULL(MAX(Engagement.bounce_rate), 0) AS bounce_rate,\n",
        "IFNULL(MAX(Engagement.bounces), 0) AS bounces,\n",
        "IFNULL(MAX(Engagement.engagement_time_seconds), 0) AS engagement_time_seconds,\n",
        "IFNULL(MAX(Engagement.total_sessions), 0) AS total_sessions,\n",
        "MAX(Pages_Tracking.page) as page,\n",
        "MAX(Pages_Tracking.pagepath_level_1) as pagepath_level_1,\n",
        "MAX(Pages_Tracking.previous_page) as previous_page,\n",
        "MAX(Pages_Tracking.previous_page_path_level_1) as previous_page_path_level_1,\n",
        "MAX(Pages_Tracking.landing_page) as landing_page,\n",
        "MAX(Pages_Tracking.landing_pagepath_level_1) as landing_pagepath_level_1,\n",
        "MAX(Pages_Tracking.second_page) as second_page,\n",
        "MAX(Pages_Tracking.second_pagepath_level_1) as second_pagepath_level_1,\n",
        "MAX(Pages_Tracking.exit_page) exit_page,\n",
        "MAX(Pages_Tracking.exit_pagepath_level_1) as exit_pagepath_level_1,\n",
        "MAX(Pages_Tracking.entrances) as entrances,\n",
        "MAX(Pages_Tracking.pageviews) as pageviews,\n",
        "MAX(Pages_Tracking.unique_pageviews) as unique_pageviews,\n",
        "MAX(Pages_Tracking.pages_per_session) as pages_per_session,\n",
        "MAX(Pages_Tracking.exits) as exits,\n",
        "MAX(Pages_Tracking.exit_rate) as exit_rate,\n",
        "MAX(CASE ep.key\n",
        "   WHEN \"tax\" THEN CAST(ep.value.double_value AS STRING)\n",
        "   END)\n",
        "AS tax,\n",
        "MAX(CASE ep.key\n",
        "   WHEN \"ga_session_id\" THEN CAST(ep.value.int_value AS STRING)\n",
        "   END)\n",
        "AS ga_session_id,\n",
        "CASE WHEN MAX(CASE ep.key WHEN \"ga_session_number\" THEN ep.value.int_value END) = 1 THEN 'new' ELSE 'return' END\n",
        "AS new_or_returning_visitor,\n",
        "#MAX(CASE ep.key\n",
        "#    WHEN \"engagement_time_msec\" THEN CAST(ep.value.int_value AS STRING)\n",
        "#    END)\n",
        "#AS engagement_time_msec,\n",
        "MAX(CASE ep.key\n",
        "   WHEN \"shipping_tier\" THEN CAST(ep.value.string_value AS STRING)\n",
        "   END)\n",
        "AS shipping_tier,\n",
        "MAX(CASE ep.key\n",
        "   WHEN \"coupon\" THEN CAST(ep.value.string_value AS STRING)\n",
        "   END)\n",
        "AS coupon,\n",
        "MAX(CASE ep.key\n",
        "   WHEN \"promotion_name\" THEN CAST(ep.value.string_value AS STRING)\n",
        "   END)\n",
        "AS promotion_name,\n",
        "MAX(CASE ep.key\n",
        "   WHEN \"payment_type\" THEN CAST(ep.value.string_value AS STRING)\n",
        "   END)\n",
        "AS payment_type,\n",
        "MAX(CASE ep.key\n",
        "   WHEN \"campaign\" THEN CAST(ep.value.string_value AS STRING)\n",
        "   END)\n",
        "AS campaign,\n",
        "COUNT(CASE ep.key\n",
        "   WHEN \"engaged_session_event\" THEN CAST(ep.value.int_value AS STRING)\n",
        "   END)\n",
        "AS engaged_session_event,\n",
        "MAX(A.event_value_in_usd) as event_value_in_usd,\n",
        "MAX(timestamp_micros(A.user_first_touch_timestamp)) as user_first_touch_timestamp,\n",
        "MAX(A.user_ltv.revenue) as ltv_revenue,\n",
        "MAX(A.device.category) as category,\n",
        "MAX(A.device.mobile_brand_name) as mobile_brand_name,\n",
        "MAX(A.device.operating_system) as operating_system,\n",
        "MAX(A.device.operating_system_version) as operating_system_version,\n",
        "MAX(A.device.web_info.browser) as browser,\n",
        "MAX(A.device.web_info.browser_version) as browser_version,\n",
        "MAX(A.geo.continent) as continent,\n",
        "MAX(A.geo.sub_continent) as sub_continent,\n",
        "MAX(A.geo.country) as country,\n",
        "MAX(A.geo.region) as region,\n",
        "MAX(A.geo.city) as city,\n",
        "MAX(A.traffic_source.medium) as traffic_medium,\n",
        "MAX(A.traffic_source.source) as traffic_source,\n",
        "CAST(ROUND(AVG(it.quantity)) AS INT64) as avg_item_quantity,\n",
        "FROM (\n",
        " select e.* EXCEPT(event_params), ep\n",
        " FROM `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events*` e , UNNEST(event_params) ep\n",
        ") A, UNNEST(A.items) it\n",
        "LEFT JOIN repeated_purchase as Repeated_Purchased\n",
        "ON A.user_pseudo_id = Repeated_Purchased.user_pseudo_id\n",
        "LEFT JOIN engagement AS Engagement\n",
        "ON A.user_pseudo_id = Engagement.user_pseudo_id AND A.event_date = Engagement.event_date\n",
        "LEFT JOIN user_events_counted as User_Events_Counted\n",
        "ON A.user_pseudo_id = User_Events_Counted.user_pseudo_id\n",
        "LEFT JOIN pages_tracking AS Pages_Tracking\n",
        "ON A.user_pseudo_id = Pages_Tracking.user_pseudo_id AND A.event_date = Pages_Tracking.event_date\n",
        "WHERE\n",
        "ep.key IN ('tax', 'ga_session_id', 'ga_session_number',\n",
        "               'engagement_time_msec', 'shipping_tier',\n",
        "               'coupon', 'promotion_name', 'payment_type',\n",
        "               'page_location', 'campaign', 'engaged_session_event')\n",
        "AND parse_date(\"%Y%m%d\",A.event_date) >= parse_date(\"%Y-%m-%d\", '2021-01-01')\n",
        "GROUP BY user_pseudo_id, A.event_date)\n",
        "\n",
        "SELECT\n",
        "*\n",
        "EXCEPT(page, previous_page, landing_page, second_page,\n",
        "        exit_page, exit_pagepath_level_1, ga_session_id,\n",
        "        event_value_in_usd, user_first_touch_timestamp, count_item_purchases,\n",
        "        tax, shipping_tier, coupon, promotion_name, payment_type)\n",
        "        #store_code, store_name, transaction_channel,\n",
        "        #engagement_time_msec\n",
        "        #item_category5, google_product_category_path, product_gtin)\n",
        "FROM master_ga4_erp\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b137-Ob6k-nD",
      "metadata": {
        "id": "b137-Ob6k-nD"
      },
      "source": [
        "Combining both the queries, to execute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5Gu17Mrtp0N",
      "metadata": {
        "id": "e5Gu17Mrtp0N"
      },
      "outputs": [],
      "source": [
        "final_feature_table = f\"\"\"\n",
        "{base_feature_tabel}{grouped_feature_query}\n",
        "\"\"\"\n",
        "# print(final_feature_table)\n",
        "query_job = client.query(final_feature_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hrJCerh-ay8C",
      "metadata": {
        "id": "hrJCerh-ay8C"
      },
      "source": [
        "The above feature creation takes approx 5 minutes. The following code will throw an error unless the table has been built. Wait and retry. If you still get an error after a while, your query might have failed to execute. Try running the query in the BQ console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EsYLSb48KIHP",
      "metadata": {
        "id": "EsYLSb48KIHP"
      },
      "outputs": [],
      "source": [
        "query = f\"\"\"\n",
        "\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  `{DATASET_NAME}.{feature_table}`\n",
        "LIMIT\n",
        "5\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "result_df = query_job.to_dataframe()\n",
        "result_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I4i8g9LDc5RA",
      "metadata": {
        "id": "I4i8g9LDc5RA"
      },
      "source": [
        "## BQML Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-vXqjW9TvPTj",
      "metadata": {
        "id": "-vXqjW9TvPTj"
      },
      "source": [
        "Once we have all the features, we can run the BQML model. We don't have to specify specific parameters and can leverage auto hyperparameter tunning. Just as we discussed, our target variable is \"ltv_revenue\". We want the model to understand the relationship between all the user-specific features and \"ltv_revenue\", so we can use this model to predict future LTV values for customers. The model can also help us predict for users where the default LTV (given in the dashboard) failed to generate any LTV.\n",
        "\n",
        "We can also leverage global explaination available as part of BQML by using enable_global_explain=TRUE. This can provide us with feature rankings and their weights. It will also help us identify why each prediction is being made and which columns have been responsible for the predicted value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZDybb73d3Ht5",
      "metadata": {
        "id": "ZDybb73d3Ht5"
      },
      "outputs": [],
      "source": [
        "model_name = \"customer_ltv_model\"\n",
        "trails = 8\n",
        "\n",
        "linear_regression_query = f\"\"\"\n",
        "CREATE OR REPLACE MODEL\n",
        "  `{DATASET_NAME}.{model_name}` OPTIONS (model_type='linear_reg',\n",
        "    input_label_cols=['ltv_revenue'],num_trials={trails},\n",
        "   max_parallel_trials=4,\n",
        "   enable_global_explain=TRUE) AS\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  `{PROJECT_ID}.{DATASET_NAME}.{feature_table}`\n",
        "WHERE\n",
        "  ltv_revenue IS NOT NULL\n",
        "\"\"\"\n",
        "# print(linear_regression_query)\n",
        "query_job = client.query(linear_regression_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GeoJwWqHxhQt",
      "metadata": {
        "id": "GeoJwWqHxhQt"
      },
      "outputs": [],
      "source": [
        "model_name = \"customer_ltv_model\"\n",
        "ml_evaluate_query = f\"\"\"\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  ML.EVALUATE(MODEL `{DATASET_NAME}.{model_name}`,\n",
        "    (\n",
        "    SELECT\n",
        "      *\n",
        "    FROM\n",
        "      `{PROJECT_ID}.{DATASET_NAME}.{feature_table}`\n",
        "    WHERE\n",
        "      ltv_revenue IS NOT NULL))\n",
        "\"\"\"\n",
        "query_job = client.query(ml_evaluate_query)\n",
        "ml_info_df = query_job.to_dataframe()\n",
        "ml_info_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dHULDTnhwvJD",
      "metadata": {
        "id": "dHULDTnhwvJD"
      },
      "source": [
        "After a while, we can check the final results of tunning. We can observe that we already have a model with 71% r2 and 72% variance explained. These are not great numbers, but they are good to start. We only have three months of user data to manage our expectations from an accuracy perspective. In the future, when more data flow, we can expect it to explain more than 90% variance, such that we have reliable LTV."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0gPQNmhwwel",
      "metadata": {
        "id": "e0gPQNmhwwel"
      },
      "source": [
        "Since we have asked the BQML model to run different trials, we can check the parameters for each trial to decide on any other model if we want to. However, by default, it has picked the first model, where is_optimal = True."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tJ8HzP6Py142",
      "metadata": {
        "id": "tJ8HzP6Py142"
      },
      "outputs": [],
      "source": [
        "ml_trail_info_query = f\"\"\"\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  ML.TRIAL_INFO(MODEL `{DATASET_NAME}.{model_name}`)\n",
        "\"\"\"\n",
        "query_job = client.query(ml_trail_info_query)\n",
        "ml_trail_df = query_job.to_dataframe()\n",
        "ml_trail_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nvoChYnuc_Af",
      "metadata": {
        "id": "nvoChYnuc_Af"
      },
      "source": [
        "## Batch Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TAO7zsiWyf9g",
      "metadata": {
        "id": "TAO7zsiWyf9g"
      },
      "source": [
        "Now that we have the final model, we can use our data to predict the new values for LTV, which we are considering more comprehensive and future values. Do note that we have not broken our data into train and test and our just using the whole feature set as training data. That is not an optimized way to execute. So, we are sending the same features in the model for prediction. The LTV revenue will be considered as the future values for each user.\n",
        "\n",
        "You can also carefully focus on users where the ltv_revenue is zero, but our models have given some positive values. This shows that model thinks that these users have some potential and can be used for more targeted marketing.\n",
        "\n",
        "Some of the same users with zero base values are also given negative values, which can be considered zero - meaning the model also thinks they have no potential revenue in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nxjJ9F9RAa6n",
      "metadata": {
        "id": "nxjJ9F9RAa6n"
      },
      "outputs": [],
      "source": [
        "prediction_data_table_name = \"model_prediction_ltv\"\n",
        "query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE\n",
        "  {DATASET_NAME}.{prediction_data_table_name} AS\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  ML.PREDICT(MODEL `{DATASET_NAME}.{model_name}`,\n",
        "    (\n",
        "    SELECT\n",
        "      *\n",
        "    FROM\n",
        "      `{PROJECT_ID}.{DATASET_NAME}.{feature_table}`\n",
        "    WHERE\n",
        "      ltv_revenue IS NOT NULL\n",
        "    ))\n",
        "\"\"\"\n",
        "# print(query)\n",
        "query_job = client.query(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mz8_PxJnOPxy",
      "metadata": {
        "id": "mz8_PxJnOPxy"
      },
      "outputs": [],
      "source": [
        "query = f\"\"\"\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  {PROJECT_ID}.{DATASET_NAME}.{prediction_data_table_name}\n",
        "LIMIT 1000\n",
        "\"\"\"\n",
        "# print(query)\n",
        "query_job = client.query(query)\n",
        "predict_data = query_job.to_dataframe()\n",
        "predict_data[predict_data[\"predicted_ltv_revenue\"] >= 0.0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zysINzjezewg",
      "metadata": {
        "id": "zysINzjezewg"
      },
      "outputs": [],
      "source": [
        "predict_data[predict_data[\"predicted_ltv_revenue\"] >= 0.0][\n",
        "    [\"user_pseudo_id\", \"predicted_ltv_revenue\", \"ltv_revenue\"]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bgUVx0uxASZd",
      "metadata": {
        "id": "bgUVx0uxASZd"
      },
      "source": [
        "## Prediction Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ttvNhnr1h9z",
      "metadata": {
        "id": "2ttvNhnr1h9z"
      },
      "source": [
        "Since we have used a global explanation in our model, we can see the weights of features. They are already ranked based on their importance. As you can see:  new_or_returning_visitor, has_purchased, and count_add_to_cart_item are the top 3 most important feature that helps the model predict LTV. This is also true from a real-world perspective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p5eBbra-McKk",
      "metadata": {
        "id": "p5eBbra-McKk"
      },
      "outputs": [],
      "source": [
        "query = f\"\"\"\n",
        "#standardSQL\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  ML.GLOBAL_EXPLAIN(MODEL `{DATASET_NAME}.{model_name}`)\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "features_weight = query_job.to_dataframe()\n",
        "features_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rGCWrC_K2tTx",
      "metadata": {
        "id": "rGCWrC_K2tTx"
      },
      "source": [
        "At last, we can take this further and ask the model to give local explanations. It will provide row-level or user-level descriptions of essential features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ysmracjiM-0i",
      "metadata": {
        "id": "ysmracjiM-0i"
      },
      "outputs": [],
      "source": [
        "query = f\"\"\"\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  ML.EXPLAIN_PREDICT(MODEL `{DATASET_NAME}.{model_name}`,\n",
        "    (\n",
        "    SELECT\n",
        "      *\n",
        "    FROM\n",
        "      `{PROJECT_ID}.{DATASET_NAME}.{feature_table}`\n",
        "    WHERE\n",
        "      ltv_revenue IS NOT NULL\n",
        "      ),\n",
        "    STRUCT(3 as top_k_features))\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "explain_prediction_df = query_job.to_dataframe()\n",
        "# explain_prediction_df.head()\n",
        "columns_to_view = [\n",
        "    \"user_pseudo_id\",\n",
        "    \"predicted_ltv_revenue\",\n",
        "    \"ltv_revenue\",\n",
        "    \"top_feature_attributions\",\n",
        "    \"baseline_prediction_value\",\n",
        "    \"approximation_error\",\n",
        "]\n",
        "explain_prediction_df[explain_prediction_df[\"predicted_ltv_revenue\"] > 10.0][\n",
        "    columns_to_view\n",
        "].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EK3_jiBP3q5a",
      "metadata": {
        "id": "EK3_jiBP3q5a"
      },
      "source": [
        "We can take the example of one user - '88839956.1260646312'\n",
        "You can see that the default LTV revenue was zero, and the model predicted 12.79.\n",
        "\n",
        "We can see why the user has some potential LTV since the user is a new_or_returning_visitor and the count_days of views are higher. Both the columns have boosted the user LTV and our model thinks that the user holds an excellent 12$ potential."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uECAEPJU2FK_",
      "metadata": {
        "id": "uECAEPJU2FK_"
      },
      "outputs": [],
      "source": [
        "explain_prediction_df[explain_prediction_df[\"user_pseudo_id\"] == \"88839956.1260646312\"][\n",
        "    [\n",
        "        \"user_pseudo_id\",\n",
        "        \"predicted_ltv_revenue\",\n",
        "        \"ltv_revenue\",\n",
        "        \"top_feature_attributions\",\n",
        "        \"baseline_prediction_value\",\n",
        "        \"approximation_error\",\n",
        "    ]\n",
        "].iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hkvyyyOg2m-G",
      "metadata": {
        "id": "hkvyyyOg2m-G"
      },
      "outputs": [],
      "source": [
        "explain_prediction_df[explain_prediction_df[\"user_pseudo_id\"] == \"88839956.1260646312\"][\n",
        "    [\n",
        "        \"user_pseudo_id\",\n",
        "        \"predicted_ltv_revenue\",\n",
        "        \"ltv_revenue\",\n",
        "        \"top_feature_attributions\",\n",
        "        \"baseline_prediction_value\",\n",
        "        \"approximation_error\",\n",
        "    ]\n",
        "].iloc[0][\"top_feature_attributions\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MneZoCsCAjeT",
      "metadata": {
        "id": "MneZoCsCAjeT"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "Once you have the model ready, there can be multiple things that can be achieved through LTV\n",
        "\n",
        "\n",
        "1. Periodic LTV Monitoring:\n",
        "\n",
        "Monitoring your performance is a straightforward use of Future LTV that may be used. It is advisable to run the LTV calculation at least once every month and compare your performance to determine the efficacy of your marketing initiatives, in terms of increasing the lifetime value of your clients.\n",
        "\n",
        "2. Develop your marketing approach\n",
        "\n",
        "It is possible to study the impact that a number of different variables have on the average lifespan of a customer by using a number of different \"if-then\" scenarios. You are able to determine which factors may be altered in order to bring about a different response from the target audience. For instance, you may discover that lowering the price of your items not only encourages people to purchase from you more frequently but also results in a lower turnover rate among those customers. You can improve the worth of your consumers during their lifespan by lowering the prices of certain of your items, which you can do now that you have this additional knowledge.\n",
        "\n",
        "3. Determine which of your marketing avenues bring in the most revenue.\n",
        "\n",
        "You are able to examine LTV on a channel-by-channel, campaign-by-campaign, source-by-source, and medium-by-medium basis by using the reports that Google Analytics provides. LTV is able to inform you whether or not you are spending too little or too much money on each of these channels. The following is an excellent illustration of this: If you determine that the LTV of email is x dollar, but the LTV of a Facebook channel is x+100 dollar, then you should spend twice as much time on Facebook as you do on email since you know the Facebook channel is more valuable.\n",
        "\n",
        "4. Establish a customer-loyalty programme.\n",
        "\n",
        "You are able to divide your consumers into groups according to the amount of money they are expected to spend over the course of their lifetime, and then tailor the way you communicate with each of those groups. Keeping this in mind, you may devise a customer loyalty programme that tailors its communications and offers of enticement to the specific needs of various subsets of your clientele, while also continuing to track and evaluate the program's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdeJo1Q7d-xY",
      "metadata": {
        "id": "bdeJo1Q7d-xY"
      },
      "source": [
        "## Clean Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vFSfNFYMeFIN",
      "metadata": {
        "id": "vFSfNFYMeFIN"
      },
      "outputs": [],
      "source": [
        "# # Are you sure you want to do this? This is to delete all models\n",
        "# models = client.list_models(dataset_id) # Make an API request.\n",
        "# for model in models:\n",
        "#     full_model_id = f\"{model.dataset_id}.{model.model_id}\"\n",
        "#     client.delete_model(full_model_id)  # Make an API request.\n",
        "#     print(f\"Deleted: {full_model_id}\")\n",
        "# # Are you sure you want to do this? This is to delete all tables and views\n",
        "# tables = client.list_tables(dataset_id)  # Make an API request.\n",
        "# for table in tables:\n",
        "#     full_table_id = f\"{table.dataset_id}.{table.table_id}\"\n",
        "#     client.delete_table(full_table_id)  # Make an API request.\n",
        "#     print(f\"Deleted: {full_table_id}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MneZoCsCAjeT"
      ],
      "name": "GA4_Long_Term_Value_Future_Prediction.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
